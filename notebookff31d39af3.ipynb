{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13128808,"sourceType":"datasetVersion","datasetId":8317056}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\"\"\"\n.\nconvert x and y to numpy arrays here\n.\n\"\"\"\nX=np.array(X)import os, numpy as np, matplotlib.pyplot as plt\nfolder = \"/kaggle/input/fashion-mnist/final\" # folder containing the data\n\ndef load_data(X, y):\n    for f in os.listdir(folder):\n        for file in os.listdir(f\"{folder}/{f}\"):\n            img = plt.imread(f\"{folder}/{f}/{file}\")\n            X.append(img)\n\n            # The most obvious choice for the label is the class (folder) name\n            # label = int(f)\n\n            # [Q1] But we dont use it here why? Why is it an array of 10 elements?\n            # Clue: Lookup one hot encoding\n            # Read up on Cross Entropy Loss\n\n            label = [0] * 10\n            label[int(f)] = 1 # Why is this array the label and not a numeber?\n\n            y.append(label)\n            \n        print(f\"Loaded {f} class\")\n\nX, y = [], []\nload_data(X, y)\n\n# [Q2] Why convert to numpy array?\ny=np.array(y)\n\nX = X[:, :,:, 0] # [Q3] Why are we doing this and what does this type of slicing result in?\nX = X.reshape(X.shape[0], X.shape[1]*X.shape[2]) # [Q4] Why are we reshaping the data?\nprint(\"After reshaping\")\nprint(X.shape, y.shape)\nprint(X[0], y[0])\n\nclass NN:\n    def __init__(self, input_neurons, hidden_neurons, output_neurons, learning_rate, epochs):\n        \"\"\"\n        Class Definition\n        \n        We use a class because it is easy to visualize the process of training a neural network\n        It's also easier to resuse and repurpose depending on the task at hand\n\n        We have a simple neural network, with an input layer, one hidden (middle) layer and an output layer\n\n        input_neurons: Number of neurons in the input layer\n        hidden_neurons: Number of neurons in the hidden layer\n        output_neurons: Number of neurons in the output layer\n        learning_rate: The rate at which the weights are updated [Q5] What is the learning rate?\n        epochs: Number of times the model will train on the entire dataset \n        \"\"\"\n\n        self.input_neurons = input_neurons\n        self.hidden_neurons = hidden_neurons\n        self.output_neurons = output_neurons\n        self.epochs = epochs\n\n        self.lr = learning_rate\n\n        \"\"\"\n        Weights and Biases\n\n        At this point you should know what weights and biases are in a neural network and if not, go check out the 3blue1brown video on Neural Networks\n        What matters here is however the matrix dimensions of the weights and biases\n\n         [Q6] Why are the dimensions of the weights and biases the way they are? \n        \n        Try to figure out the dimensions of the weights and biases for the hidden and output layers\n        Try to see what equations represent the forward pass (basically the prediction)\n        And then, try to see if the dimensions of the matrix multiplications are correct\n\n        Note: The bias dimensions may not match. Look up broadcasting in numpy to understand\n        [Q7] What is broadcasting and why do we need to broadcast the bias?\n        \"\"\"\n\n        # Ideally any random set of weights and biases can be used to initialize the network\n        # self.wih = np.random.randn(hidden_neurons, input_neurons)\n\n        # [Q8] What is np.random.randn? What's the shape of this matrix?\n\n        # Optional: Try to figure out why the weights are initialized this way\n        # Note: You can just use the commented out line above if you don't want to do this\n\n        self.wih = np.random.randn(hidden_neurons, input_neurons) * np.sqrt(2/input_neurons)\n        self.bih = np.zeros((hidden_neurons, 1))\n\n        self.who = np.random.randn(output_neurons, hidden_neurons) * np.sqrt(2/hidden_neurons)\n        self.bho = np.zeros((output_neurons, 1))\n\n    # Activation Functions and their derivatives\n    # [Q9] What are activation functions and why do we need them?\n\n    def relu(self, z):\n        \"\"\"\n        Implementation of the RELU function\n        z: (n, 1)\n        returns (n, 1)\n        \"\"\"\n        return z * (z > 0)\n\n    def sigmoid(self, z):\n        \"\"\"\n        Implementation of the Sigmoid function\n        z: (n, 1)\n        returns (n, 1)\n        \"\"\"\n        return 1 / (1 + np.exp(-z))\n\n    def relu_derivative(self, z):\n        \"\"\"\n        Implementation of the RELU derivative function\n        z: (n, 1)\n        returns (n, 1)\n        \"\"\"\n        return 1 * (z > 0)\n\n    def sigmoid_derivative(self, z):\n        \"\"\"\n        Implementation of the Sigmoid derivative function\n        z: (n, 1)\n        returns (n, 1)\n        \"\"\"\n        return z * (1 - z)\n\n    # [Q10] What is the softmax function and why do we need it? Read up on it\n    def softmax(self, z):\n        \"\"\"\n        Implementation of the Softmax function\n        z: (n, 1)\n        returns (n, 1)\n        \"\"\"\n        return np.exp(z) / np.sum(np.exp(z), axis=0)\n\n    def softmax_derivative(self, z):\n        \"\"\"\n        Implementation of the Softmax derivative function\n        z: (n, 1)\n        returns (n, 1)\n        \"\"\"\n        return z * (1 - z)\n\n    # Loss Functions and their derivatives\n    # [Q11] What are loss functions and why do we need them?\n\n    def mean_squared_error(self, y, y_hat):\n        \"\"\"\n        Implementation of the Mean Squared Error function\n        y: (10, n)\n        y_hat: (10, n)\n        returns (1, n)\n        \"\"\"\n        return np.mean((y - y_hat) ** 2, axis=0)\n\n    def cross_entropy_loss(self, y, y_hat):\n        \"\"\"\n        Implementation of the Cross Entropy Loss function\n        y: (10, n)\n        y_hat: (10, n)\n        returns (1, n)\n        \"\"\"\n\n        # Implement the cross entropy loss function here and return it\n        # Keep the dimensions of the input in mind when writing the code\n\n        # [Code Goes Here]\n    logits , batch_size = y.shape\n    loss = []\n    epsilon = 1e-9  # I got runtime error which I debugged and found out this log(0) problem so had to declare this using help of internet\n    \n    for i in range(batch_size):\n        t = 0\n        for j in range(logits):\n            t += -y[j, i] * np.log(y_hat[j, i] + epsilon)\n        loss.append(t)\n    loss = np.array([loss])\n    return loss\n    \n    def mean_squared_error_derivative(self, y, y_hat):\n        \"\"\"\n        Implementation of the Mean Squared Error derivative function\n        y: (10, n)\n        y_hat: (10, n)\n        returns (10, n)\n        \"\"\"\n        return y_hat - y\n\n    def cross_entropy_derivative(self, y, y_hat):\n        \"\"\"\n        Implementation of the Cross Entropy Loss derivative function\n        y: (10, n)\n        y_hat: (10, n)\n        returns (10, n)\n        \"\"\"\n\n        # Implement the cross entropy loss derivative function here and return it\n        # Note: The derivative of the CEL is usually taken with respect to the softmax input not output so keep that in mind while writing\n\n        # [Code Goes Here]\n        cross_entropy_der = y_hat - y\n\n        return cross_entropy_der\n    # Forward propagation\n    def forward(self, input_list):\n        \"\"\"\n        Implementation of the Forward Pass\n        input_list: (784, n)        - n is the number of images\n        returns (10, n)              - n is the number of images\n    \n        Now we come to the heart of the neural network, the forward pass\n        This is where the input is passed through the network to get the output\n\n        [Q12] What does the output choice we have here mean? It's an array of 10 elements per image, but why?\n        \"\"\"\n\n        inputs = np.array(input_list, ndmin=2).T\n        inputs = inputs - np.mean(inputs) # [Q13] Why are we subtracting the mean of the inputs?\n        \n        # To get to the hidden layer:\n        # Multiply the input with the weights and adding the bias\n        # Apply the activation function (relu in this case)\n\n        # [Code Goes Here]\n        z_hidden = np.dot(self.wih, inputs) + self.bih\n        a_hidden = self.relu(z_hidden)\n\n\n        # To get to the output layer:\n        # Multiply the hidden layer output with the weights and adding the bias\n        # Apply the activation function (softmax in this case)\n        # [Q14] Why are we using the softmax function here?\n\n        # [Code Goes Here]\n        z_output = np.dot(self.who, a_hidden) + self.bho\n        y_hat = self.softmax(z_output)\n\n\n        # Return it\n\n        # [Code Goes Here]\n        return y_hat\n        \n\n    # Back propagation\n    def backprop(self, inputs_list, targets_list):\n        \"\"\"\n        Implementation of the Backward Pass\n        inputs_list: (784, n)\n        targets_list: (10, n)\n        returns a scalar value (loss)\n        \n        This is where the magic happens, the backpropagation algorithm\n        This is where the weights are updated based on the error in the prediction of the network\n\n        Now, the calculus involved is fairly complicated, especially because it's being done in matrix form\n        However the intuition is simple. \n\n        Since this is a recruitment stage, most of the function is written out for you, so follow along with the comments\n        \"\"\"\n\n        # Basic forward pass to get the outputs\n        # Obviously we need the predictions to know how the model is doing\n        # [Q15] Why are we doing a forward pass here instead of just using the outputs from the forward function?\n        # Is there any actual reason, or could we just swap it?\n\n        inputs = np.array(inputs_list, ndmin=2).T # (784, n)\n        inputs = inputs - np.mean(inputs)\n\n        tj = np.array(targets_list, ndmin=2).T # (10, n)\n\n        hidden_inputs = np.dot(self.wih, inputs) + self.bih\n        hidden_outputs = self.relu(hidden_inputs)\n\n        final_inputs = np.dot(self.who, hidden_outputs) + self.bho\n        yj = self.softmax(final_inputs)\n\n        # Calculating the loss - This is the error in the prediction\n        # The loss then is the indication of how well the model is doing, its a useful parameter to track to see if the model is improving\n\n        loss = self.cross_entropy_loss(tj, yj) # Convert this to cross entropy loss\n\n\n        # Updating the weights using Update Rule\n        # Now that we have the incorrect predictions, we can update the weights to make the predictions better\n        # This is done using the gradient of the loss function with respect to the weights\n        # Basically, we know how much the overall error is caused due to individual weights using the chain rule of calculus\n        # Since we want to minimise the error, we move in the opposite direction of something like a \"derivative\" of the error with respect to the weights\n        # Calculus therefore helps us find the direction in which we should move to reduce the error\n        # A direction means what delta W changes we need to make to make the model better\n\n        # Output Layer - We start with the output layer because we are backtracking how the error is caused\n        # Think of it as using the derivatives of each layer while going back\n\n\n        # For the task, you will be using Cross Entropy Loss\n\n        # Change this to cross entropy loss\n        dE_dzo = self.mean_squared_error_derivative(tj, yj) * self.softmax_derivative(yj) # (10,n)\n        # Note: the derivative of the CEL is usually taken with respect to the softmax input not output so keep that in mind while writing\n\n\n        dE_dwho = np.dot(dE_dzo, hidden_outputs.T) / hidden_outputs.shape[1] # dot((10,n) (n,128) = (10,128)\n        dE_dbho = np.mean(dE_dzo, axis=1, keepdims=True) # sum((10,n), axis=1) = (10,1)\n        \n        self.who -= self.lr * dE_dwho\n        self.bho -= self.lr * dE_dbho\n\n        # Hidden Layer\n        dE_dah = np.dot(self.who.T, dE_dzo) # dot((128,10), (10,n)) = (128,n)\n        dE_dzh = dE_dah * self.relu_derivative(hidden_inputs)\n        dE_dwih = np.dot(dE_dzh, inputs.T) / inputs.shape[1]\n        dE_dbih = np.mean(dE_dzh, axis=1, keepdims=True)\n\n\n        self.wih -= self.lr * dE_dwih\n        self.bih -= self.lr * dE_dbih\n\n        return np.mean(loss)\n\n    def fit(self, inputs_list, targets_list,validation_data, validation_labels):\n        \"\"\"\n        Implementation of the training loop\n        inputs_list: (784, n)\n        targets_list: (10, n)\n        validation_data: (784, n)\n        validation_labels: (10, n)\n        returns train_loss, val_loss\n\n        This is where the training loop is implemented\n        We loop over the entire dataset for a certain number of epochs\n        We also track the validation loss to see how well the model is generalizing\n        [Q16] What is the validation dataset and what do we mean by generalization?\n\n        We also return the training and validation loss to see how the model is improving\n        It's a good idea to plot these to see how the model is doing\n        \"\"\"\n\n        train_loss = []\n        val_loss = []\n        for epoch in range(self.epochs):\n            loss = self.backprop(inputs_list, targets_list)\n            train_loss.append(loss)\n            vloss = self.mean_squared_error(validation_labels.T, self.forward(validation_data))\n            val_loss.append(np.mean(vloss)) \n            print(f\"Epoch: {epoch}, Loss: {loss}, Val Loss: {val_loss[-1]}\")\n\n        return train_loss[1:], val_loss[:-1] \n\n    def predict(self, X):\n        outputs = self.forward(X).T\n        return outputs\n\n# This is where the class is used to train the model\n\n# The parameters in the model are (input_neurons, hidden_neurons, output_neurons, learning_rate, epochs)\n# These parameters aren't the right parameters, so tweak them to get the best results\n# Around 70% accuracy is a good end goal (75% is great) but for the recruitment task, 60% is good enough\n\n# [Q17] What are the parameters in the model and what do they mean?\n\nfashion_mnist = NN(784, 160, 10, 0.03, 130)\np = np.random.permutation(len(X))\nX, y = X[p], y[p]\n\n# Splitting the data into training, validation and testing in the ratio 70:20:10\nX_train, y_train = X[:int(0.7*len(X))], y[:int(0.7*len(X))]\nX_val, y_val = X[int(0.7*len(X)):int(0.9*len(X))], y[int(0.7*len(X)):int(0.9*len(X))]\nX_test, y_test = X[int(0.9*len(X)):], y[int(0.9*len(X)):]\n\n# Training the model\ntrain_loss,val_loss = fashion_mnist.fit(X_train, y_train,X_val,y_val)\n\n\n# Plotting the loss\nplt.plot(train_loss,label='train')\nplt.plot(val_loss,label='val')\nplt.legend()\nplt.show()\n\ny_pred = fashion_mnist.predict(X_test)\n\n# [Q18] Why are we using argmax here? Why is this output different from the output of the model?\ny_pred = np.argmax(y_pred, axis=1)  \ny_test = np.argmax(y_test, axis=1)\nprint(f\"Accuracy: {np.mean(y_pred == y_test)}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-21T18:14:13.038693Z","iopub.execute_input":"2025-09-21T18:14:13.039000Z","iopub.status.idle":"2025-09-21T18:17:26.061805Z","shell.execute_reply.started":"2025-09-21T18:14:13.038979Z","shell.execute_reply":"2025-09-21T18:17:26.060712Z"}},"outputs":[{"name":"stdout","text":"Loaded 7 class\nLoaded 2 class\nLoaded 5 class\nLoaded 8 class\nLoaded 0 class\nLoaded 3 class\nLoaded 1 class\nLoaded 4 class\nLoaded 9 class\nLoaded 6 class\nAfter reshaping\n(60000, 784) (60000, 10)\n[0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.00392157 0.         0.         0.1254902  0.1882353\n 0.         0.52156866 0.28627452 0.         0.         0.\n 0.         0.         0.         0.01568628 0.1254902  0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.01960784 0.\n 0.00392157 0.15686275 0.5254902  0.7019608  0.5294118  0.4627451\n 0.69803923 0.25490198 0.         0.01960784 0.         0.05882353\n 0.1882353  0.2509804  0.5254902  0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.00392157 0.01568628 0.         0.         0.35686275 0.5137255\n 0.10588235 0.38039216 0.78431374 0.4117647  0.54509807 0.5254902\n 0.         0.         0.         0.3647059  0.7372549  0.7607843\n 0.2784314  0.         0.         0.         0.         0.\n 0.         0.         0.         0.01568628 0.         0.\n 0.07843138 0.3137255  0.5568628  0.4745098  0.09803922 0.5137255\n 0.627451   0.6745098  0.63529414 0.54901963 0.4862745  0.21176471\n 0.2509804  0.654902   0.6509804  0.69803923 0.09803922 0.\n 0.         0.         0.00392157 0.03137255 0.01960784 0.01960784\n 0.         0.         0.02745098 0.3372549  0.40784314 0.64705884\n 0.17254902 0.5921569  0.88235295 0.93333334 0.7176471  0.53333336\n 0.5686275  0.7019608  0.8039216  0.81960785 0.7647059  0.65882355\n 0.70980394 0.6901961  0.06666667 0.         0.00784314 0.\n 0.         0.         0.         0.         0.         0.14901961\n 0.36078432 0.5764706  0.28235295 0.4745098  0.6156863  0.6156863\n 0.8509804  0.8235294  0.67058825 0.8039216  0.7254902  0.6901961\n 0.59607846 0.7176471  0.8        0.7607843  0.6745098  0.7607843\n 0.09803922 0.         0.         0.         0.         0.15294118\n 0.3372549  0.4745098  0.67058825 0.70980394 0.42745098 0.38039216\n 0.61960787 0.6509804  0.50980395 0.64705884 0.6313726  0.45490196\n 0.94509804 0.654902   0.5372549  0.8901961  0.61960787 0.5686275\n 0.8117647  0.78431374 0.6862745  0.8235294  0.26666668 0.\n 0.10588235 0.6        0.2        0.5254902  0.64705884 0.58431375\n 0.5058824  0.37254903 0.47058824 0.58431375 0.56078434 0.7529412\n 0.7647059  0.27058825 0.68235296 0.4392157  0.6392157  0.8901961\n 0.75686276 0.87058824 0.8509804  0.7882353  0.7921569  0.80784315\n 0.79607844 0.7921569  0.69411767 0.         0.34901962 0.64705884\n 0.5254902  0.25490198 0.14901961 0.3254902  0.40784314 0.57254905\n 0.5372549  0.6666667  0.4745098  0.23137255 0.8        0.7137255\n 0.44313726 1.         0.7921569  0.85882354 0.8156863  0.64705884\n 0.7019608  0.7294118  0.70980394 0.74509805 0.78431374 0.73333335\n 0.80784315 0.08235294 0.36862746 0.7019608  0.6392157  0.7294118\n 0.74509805 0.69803923 0.59607846 0.56078434 0.5568628  0.5764706\n 0.7294118  0.70980394 0.7647059  0.88235295 0.8784314  0.70980394\n 0.69411767 0.5921569  0.6392157  0.65882355 0.6313726  0.6392157\n 0.5921569  0.62352943 0.627451   0.70980394 0.69411767 0.\n 0.         0.6313726  0.69411767 0.5254902  0.63529414 0.6666667\n 0.7372549  0.7137255  0.6901961  0.6156863  0.654902   0.73333335\n 0.74509805 0.68235296 0.6313726  0.64705884 0.67058825 0.73333335\n 0.8039216  0.3764706  0.28627452 0.7490196  0.7254902  0.78431374\n 0.5137255  0.5137255  0.85882354 0.17254902 0.         0.\n 0.31764707 0.35686275 0.47058824 0.9647059  0.5882353  0.34509805\n 0.67058825 0.8509804  0.5058824  0.4        0.46666667 0.73333335\n 0.91764706 0.75686276 0.7607843  0.8117647  0.69411767 0.3764706\n 0.34117648 0.6392157  0.87058824 0.7294118  0.28627452 0.34509805\n 0.74509805 0.5058824  0.00392157 0.         0.         0.06666667\n 0.09019608 0.3254902  0.3019608  0.28627452 0.45490196 0.5568628\n 0.28627452 0.3137255  0.24313726 0.24705882 0.48235294 0.\n 0.         0.         0.15686275 0.28235295 0.21176471 0.43529412\n 0.4        0.19607843 0.30588236 0.28235295 0.2        0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.        ] [0 0 0 0 0 0 0 1 0 0]\n","output_type":"stream"},{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_36/1166448011.py\"\u001b[0;36m, line \u001b[0;32m182\u001b[0m\n\u001b[0;31m    return loss\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"],"ename":"SyntaxError","evalue":"'return' outside function (1166448011.py, line 182)","output_type":"error"}],"execution_count":10}]}