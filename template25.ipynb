{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZBP3-36hMyW"
      },
      "outputs": [],
      "source": [
        "import os, numpy as np, matplotlib.pyplot as plt\n",
        "folder = \"./final/\" # folder containing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjm65g8EhMyb"
      },
      "outputs": [],
      "source": [
        "# Now we try to load the images and their corresponding labels into memory\n",
        "\n",
        "def load_data(X, y):\n",
        "    for f in os.listdir(folder):\n",
        "        for file in os.listdir(f\"{folder}/{f}\"):\n",
        "            img = plt.imread(f\"{folder}/{f}/{file}\")\n",
        "            X.append(img)\n",
        "\n",
        "            # The most obvious choice for the label is the class (folder) name\n",
        "            # label = int(f)\n",
        "\n",
        "            # [Q1] But we dont use it here why? Why is it an array of 10 elements?\n",
        "            # Clue: Lookup one hot encoding\n",
        "            # Read up on Cross Entropy Loss\n",
        "\n",
        "            label = [0] * 10\n",
        "            label[int(f)] = 1 # Why is this array the label and not a numeber?\n",
        "\n",
        "            y.append(label)\n",
        "\n",
        "        print(f\"Loaded {f} class\")\n",
        "\n",
        "X, y = [], []\n",
        "load_data(X, y)\n",
        "\n",
        "# [Q2] Why convert to numpy array?\n",
        "\"\"\"\n",
        ".\n",
        "convert x and y to numpy arrays here\n",
        ".\n",
        "\"\"\"\n",
        "\n",
        "X = X[:, :,:, 0] # [Q3] Why are we doing this and what does this type of slicing result in?\n",
        "X = X.reshape(X.shape[0], X.shape[1]*X.shape[2]) # [Q4] Why are we reshaping the data?\n",
        "print(\"After reshaping\")\n",
        "print(X.shape, y.shape)\n",
        "print(X[0], y[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-1vt3x9hMyd"
      },
      "outputs": [],
      "source": [
        "class NN:\n",
        "    def __init__(self, input_neurons, hidden_neurons, output_neurons, learning_rate, epochs):\n",
        "        \"\"\"\n",
        "        Class Definition\n",
        "\n",
        "        We use a class because it is easy to visualize the process of training a neural network\n",
        "        It's also easier to resuse and repurpose depending on the task at hand\n",
        "\n",
        "        We have a simple neural network, with an input layer, one hidden (middle) layer and an output layer\n",
        "\n",
        "        input_neurons: Number of neurons in the input layer\n",
        "        hidden_neurons: Number of neurons in the hidden layer\n",
        "        output_neurons: Number of neurons in the output layer\n",
        "        learning_rate: The rate at which the weights are updated [Q5] What is the learning rate?\n",
        "        epochs: Number of times the model will train on the entire dataset\n",
        "        \"\"\"\n",
        "\n",
        "        self.input_neurons = input_neurons\n",
        "        self.hidden_neurons = hidden_neurons\n",
        "        self.output_neurons = output_neurons\n",
        "        self.epochs = epochs\n",
        "\n",
        "        self.lr = learning_rate\n",
        "\n",
        "        \"\"\"\n",
        "        Weights and Biases\n",
        "\n",
        "        At this point you should know what weights and biases are in a neural network and if not, go check out the 3blue1brown video on Neural Networks\n",
        "        What matters here is however the matrix dimensions of the weights and biases\n",
        "\n",
        "         [Q6] Why are the dimensions of the weights and biases the way they are?\n",
        "\n",
        "        Try to figure out the dimensions of the weights and biases for the hidden and output layers\n",
        "        Try to see what equations represent the forward pass (basically the prediction)\n",
        "        And then, try to see if the dimensions of the matrix multiplications are correct\n",
        "\n",
        "        Note: The bias dimensions may not match. Look up broadcasting in numpy to understand\n",
        "        [Q7] What is broadcasting and why do we need to broadcast the bias?\n",
        "        \"\"\"\n",
        "\n",
        "        # Ideally any random set of weights and biases can be used to initialize the network\n",
        "        # self.wih = np.random.randn(hidden_neurons, input_neurons)\n",
        "\n",
        "        # [Q8] What is np.random.randn? What's the shape of this matrix?\n",
        "\n",
        "        # Optional: Try to figure out why the weights are initialized this way\n",
        "        # Note: You can just use the commented out line above if you don't want to do this\n",
        "\n",
        "        self.wih = np.random.randn(hidden_neurons, input_neurons) * np.sqrt(2/input_neurons)\n",
        "        self.bih = np.zeros((hidden_neurons, 1))\n",
        "\n",
        "        self.who = np.random.randn(output_neurons, hidden_neurons) * np.sqrt(2/hidden_neurons)\n",
        "        self.bho = np.zeros((output_neurons, 1))\n",
        "\n",
        "    # Activation Functions and their derivatives\n",
        "    # [Q9] What are activation functions and why do we need them?\n",
        "\n",
        "    def relu(self, z):\n",
        "        \"\"\"\n",
        "        Implementation of the RELU function\n",
        "        z: (n, 1)\n",
        "        returns (n, 1)\n",
        "        \"\"\"\n",
        "        return z * (z > 0)\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"\n",
        "        Implementation of the Sigmoid function\n",
        "        z: (n, 1)\n",
        "        returns (n, 1)\n",
        "        \"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def relu_derivative(self, z):\n",
        "        \"\"\"\n",
        "        Implementation of the RELU derivative function\n",
        "        z: (n, 1)\n",
        "        returns (n, 1)\n",
        "        \"\"\"\n",
        "        return 1 * (z > 0)\n",
        "\n",
        "    def sigmoid_derivative(self, z):\n",
        "        \"\"\"\n",
        "        Implementation of the Sigmoid derivative function\n",
        "        z: (n, 1)\n",
        "        returns (n, 1)\n",
        "        \"\"\"\n",
        "        return z * (1 - z)\n",
        "\n",
        "    # [Q10] What is the softmax function and why do we need it? Read up on it\n",
        "    def softmax(self, z):\n",
        "        \"\"\"\n",
        "        Implementation of the Softmax function\n",
        "        z: (n, 1)\n",
        "        returns (n, 1)\n",
        "        \"\"\"\n",
        "        return np.exp(z) / np.sum(np.exp(z), axis=0)\n",
        "\n",
        "    def softmax_derivative(self, z):\n",
        "        \"\"\"\n",
        "        Implementation of the Softmax derivative function\n",
        "        z: (n, 1)\n",
        "        returns (n, 1)\n",
        "        \"\"\"\n",
        "        return z * (1 - z)\n",
        "\n",
        "    # Loss Functions and their derivatives\n",
        "    # [Q11] What are loss functions and why do we need them?\n",
        "\n",
        "    def mean_squared_error(self, y, y_hat):\n",
        "        \"\"\"\n",
        "        Implementation of the Mean Squared Error function\n",
        "        y: (10, n)\n",
        "        y_hat: (10, n)\n",
        "        returns (1, n)\n",
        "        \"\"\"\n",
        "        return np.mean((y - y_hat) ** 2, axis=0)\n",
        "\n",
        "    def cross_entropy_loss(self, y, y_hat):\n",
        "    \"\"\"\n",
        "    Implementation of the Cross Entropy Loss function\n",
        "    y: (10, n)\n",
        "    y_hat: (10, n)\n",
        "    returns (1, n)\n",
        "    \"\"\"\n",
        "\n",
        "    # Implement the cross entropy loss function here and return it\n",
        "    # Keep the dimensions of the input in mind when writing the code\n",
        "\n",
        "    # [Code Goes Here]\n",
        "    logits, batch_size = y.shape\n",
        "    loss = []\n",
        "    epsilon = 1e-9  # I got runtime error which I debugged and found out this log(0) problem so had to declare this using help of internet\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        t = 0\n",
        "        for j in range(logits):\n",
        "            t += -y[j, i] * np.log(y_hat[j, i] + epsilon)\n",
        "        loss.append(t)\n",
        "    loss = np.array([loss])\n",
        "    return loss\n",
        "\n",
        "    def mean_squared_error_derivative(self, y, y_hat):\n",
        "        \"\"\"\n",
        "        Implementation of the Mean Squared Error derivative function\n",
        "        y: (10, n)\n",
        "        y_hat: (10, n)\n",
        "        returns (10, n)\n",
        "        \"\"\"\n",
        "        return y_hat - y\n",
        "\n",
        "    def cross_entropy_derivative(self, y, y_hat):\n",
        "        \"\"\"\n",
        "        Implementation of the Cross Entropy Loss derivative function\n",
        "        y: (10, n)\n",
        "        y_hat: (10, n)\n",
        "        returns (10, n)\n",
        "        \"\"\"\n",
        "\n",
        "        # Implement the cross entropy loss derivative function here and return it\n",
        "        # Note: The derivative of the CEL is usually taken with respect to the softmax input not output so keep that in mind while writing\n",
        "\n",
        "        # [Code Goes Here]\n",
        "        cross_entropy_der = y_hat - y\n",
        "\n",
        "        return cross_entropy_der\n",
        "    # Forward propagation\n",
        "    def forward(self, input_list):\n",
        "        \"\"\"\n",
        "        Implementation of the Forward Pass\n",
        "        input_list: (784, n)        - n is the number of images\n",
        "        returns (10, n)              - n is the number of images\n",
        "\n",
        "        Now we come to the heart of the neural network, the forward pass\n",
        "        This is where the input is passed through the network to get the output\n",
        "\n",
        "        [Q12] What does the output choice we have here mean? It's an array of 10 elements per image, but why?\n",
        "        \"\"\"\n",
        "\n",
        "        inputs = np.array(input_list, ndmin=2).T\n",
        "        inputs = inputs - np.mean(inputs) # [Q13] Why are we subtracting the mean of the inputs?\n",
        "\n",
        "        # To get to the hidden layer:\n",
        "        # Multiply the input with the weights and adding the bias\n",
        "        # Apply the activation function (relu in this case)\n",
        "\n",
        "        # [Code Goes Here]\n",
        "        z_hidden = np.dot(self.wih, inputs) + self.bih\n",
        "        a_hidden = self.relu(z_hidden)\n",
        "\n",
        "\n",
        "        # To get to the output layer:\n",
        "        # Multiply the hidden layer output with the weights and adding the bias\n",
        "        # Apply the activation function (softmax in this case)\n",
        "        # [Q14] Why are we using the softmax function here?\n",
        "\n",
        "        # [Code Goes Here]\n",
        "        z_output = np.dot(self.who, a_hidden) + self.bho\n",
        "        y_hat = self.softmax(z_output)\n",
        "\n",
        "\n",
        "        # Return it\n",
        "\n",
        "        # [Code Goes Here]\n",
        "        return y_hat\n",
        "\n",
        "\n",
        "    # Back propagation\n",
        "    def backprop(self, inputs_list, targets_list):\n",
        "        \"\"\"\n",
        "        Implementation of the Backward Pass\n",
        "        inputs_list: (784, n)\n",
        "        targets_list: (10, n)\n",
        "        returns a scalar value (loss)\n",
        "\n",
        "        This is where the magic happens, the backpropagation algorithm\n",
        "        This is where the weights are updated based on the error in the prediction of the network\n",
        "\n",
        "        Now, the calculus involved is fairly complicated, especially because it's being done in matrix form\n",
        "        However the intuition is simple.\n",
        "\n",
        "        Since this is a recruitment stage, most of the function is written out for you, so follow along with the comments\n",
        "        \"\"\"\n",
        "\n",
        "        # Basic forward pass to get the outputs\n",
        "        # Obviously we need the predictions to know how the model is doing\n",
        "        # [Q15] Why are we doing a forward pass here instead of just using the outputs from the forward function?\n",
        "        # Is there any actual reason, or could we just swap it?\n",
        "\n",
        "        inputs = np.array(inputs_list, ndmin=2).T # (784, n)\n",
        "        inputs = inputs - np.mean(inputs)\n",
        "\n",
        "        tj = np.array(targets_list, ndmin=2).T # (10, n)\n",
        "\n",
        "        hidden_inputs = np.dot(self.wih, inputs) + self.bih\n",
        "        hidden_outputs = self.relu(hidden_inputs)\n",
        "\n",
        "        final_inputs = np.dot(self.who, hidden_outputs) + self.bho\n",
        "        yj = self.softmax(final_inputs)\n",
        "\n",
        "        # Calculating the loss - This is the error in the prediction\n",
        "        # The loss then is the indication of how well the model is doing, its a useful parameter to track to see if the model is improving\n",
        "\n",
        "        loss = self.cross_entropy_loss(tj, yj) # Convert this to cross entropy loss\n",
        "\n",
        "\n",
        "        # Updating the weights using Update Rule\n",
        "        # Now that we have the incorrect predictions, we can update the weights to make the predictions better\n",
        "        # This is done using the gradient of the loss function with respect to the weights\n",
        "        # Basically, we know how much the overall error is caused due to individual weights using the chain rule of calculus\n",
        "        # Since we want to minimise the error, we move in the opposite direction of something like a \"derivative\" of the error with respect to the weights\n",
        "        # Calculus therefore helps us find the direction in which we should move to reduce the error\n",
        "        # A direction means what delta W changes we need to make to make the model better\n",
        "\n",
        "        # Output Layer - We start with the output layer because we are backtracking how the error is caused\n",
        "        # Think of it as using the derivatives of each layer while going back\n",
        "\n",
        "\n",
        "        # For the task, you will be using Cross Entropy Loss\n",
        "\n",
        "        # Change this to cross entropy loss\n",
        "        dE_dzo = self.mean_squared_error_derivative(tj, yj) * self.softmax_derivative(yj) # (10,n)\n",
        "        # Note: the derivative of the CEL is usually taken with respect to the softmax input not output so keep that in mind while writing\n",
        "\n",
        "\n",
        "        dE_dwho = np.dot(dE_dzo, hidden_outputs.T) / hidden_outputs.shape[1] # dot((10,n) (n,128) = (10,128)\n",
        "        dE_dbho = np.mean(dE_dzo, axis=1, keepdims=True) # sum((10,n), axis=1) = (10,1)\n",
        "\n",
        "        self.who -= self.lr * dE_dwho\n",
        "        self.bho -= self.lr * dE_dbho\n",
        "\n",
        "        # Hidden Layer\n",
        "        dE_dah = np.dot(self.who.T, dE_dzo) # dot((128,10), (10,n)) = (128,n)\n",
        "        dE_dzh = dE_dah * self.relu_derivative(hidden_inputs)\n",
        "        dE_dwih = np.dot(dE_dzh, inputs.T) / inputs.shape[1]\n",
        "        dE_dbih = np.mean(dE_dzh, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "        self.wih -= self.lr * dE_dwih\n",
        "        self.bih -= self.lr * dE_dbih\n",
        "\n",
        "        return np.mean(loss)\n",
        "\n",
        "    def fit(self, inputs_list, targets_list,validation_data, validation_labels):\n",
        "        \"\"\"\n",
        "        Implementation of the training loop\n",
        "        inputs_list: (784, n)\n",
        "        targets_list: (10, n)\n",
        "        validation_data: (784, n)\n",
        "        validation_labels: (10, n)\n",
        "        returns train_loss, val_loss\n",
        "\n",
        "        This is where the training loop is implemented\n",
        "        We loop over the entire dataset for a certain number of epochs\n",
        "        We also track the validation loss to see how well the model is generalizing\n",
        "        [Q16] What is the validation dataset and what do we mean by generalization?\n",
        "\n",
        "        We also return the training and validation loss to see how the model is improving\n",
        "        It's a good idea to plot these to see how the model is doing\n",
        "        \"\"\"\n",
        "\n",
        "        train_loss = []\n",
        "        val_loss = []\n",
        "        for epoch in range(self.epochs):\n",
        "            loss = self.backprop(inputs_list, targets_list)\n",
        "            train_loss.append(loss)\n",
        "            vloss = self.mean_squared_error(validation_labels.T, self.forward(validation_data))\n",
        "            val_loss.append(np.mean(vloss))\n",
        "            print(f\"Epoch: {epoch}, Loss: {loss}, Val Loss: {val_loss[-1]}\")\n",
        "\n",
        "        return train_loss[1:], val_loss[:-1]\n",
        "\n",
        "    def predict(self, X):\n",
        "        outputs = self.forward(X).T\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMqma15IhMyf"
      },
      "outputs": [],
      "source": [
        "# This is where the class is used to train the model\n",
        "\n",
        "# The parameters in the model are (input_neurons, hidden_neurons, output_neurons, learning_rate, epochs)\n",
        "# These parameters aren't the right parameters, so tweak them to get the best results\n",
        "# Around 70% accuracy is a good end goal (75% is great) but for the recruitment task, 60% is good enough\n",
        "\n",
        "# [Q17] What are the parameters in the model and what do they mean?\n",
        "\n",
        "fashion_mnist = NN(784, 160, 10, 0.03, 130)\n",
        "p = np.random.permutation(len(X))\n",
        "X, y = X[p], y[p]\n",
        "\n",
        "# Splitting the data into training, validation and testing in the ratio 70:20:10\n",
        "X_train, y_train = X[:int(0.7*len(X))], y[:int(0.7*len(X))]\n",
        "X_val, y_val = X[int(0.7*len(X)):int(0.9*len(X))], y[int(0.7*len(X)):int(0.9*len(X))]\n",
        "X_test, y_test = X[int(0.9*len(X)):], y[int(0.9*len(X)):]\n",
        "\n",
        "# Training the model\n",
        "train_loss,val_loss = fashion_mnist.fit(X_train, y_train,X_val,y_val)\n",
        "\n",
        "\n",
        "# Plotting the loss\n",
        "plt.plot(train_loss,label='train')\n",
        "plt.plot(val_loss,label='val')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "y_pred = fashion_mnist.predict(X_test)\n",
        "\n",
        "# [Q18] Why are we using argmax here? Why is this output different from the output of the model?\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_test = np.argmax(y_test, axis=1)\n",
        "print(f\"Accuracy: {np.mean(y_pred == y_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_m3p-VShMyg"
      },
      "outputs": [],
      "source": [
        "print(f\"Accuracy: {np.mean(y_pred == y_test)}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}