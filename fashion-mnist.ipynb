{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b89d7c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-22T16:44:37.200509Z",
     "iopub.status.busy": "2025-09-22T16:44:37.200271Z",
     "iopub.status.idle": "2025-09-22T16:50:50.016836Z",
     "shell.execute_reply": "2025-09-22T16:50:50.016033Z"
    },
    "papermill": {
     "duration": 372.82076,
     "end_time": "2025-09-22T16:50:50.017988",
     "exception": false,
     "start_time": "2025-09-22T16:44:37.197228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7 class\n",
      "Loaded 2 class\n",
      "Loaded 5 class\n",
      "Loaded 8 class\n",
      "Loaded 0 class\n",
      "Loaded 3 class\n",
      "Loaded 1 class\n",
      "Loaded 4 class\n",
      "Loaded 9 class\n",
      "Loaded 6 class\n",
      "After reshaping\n",
      "(60000, 784) (60000, 10)\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00392157 0.         0.         0.1254902  0.1882353\n",
      " 0.         0.52156866 0.28627452 0.         0.         0.\n",
      " 0.         0.         0.         0.01568628 0.1254902  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.01960784 0.\n",
      " 0.00392157 0.15686275 0.5254902  0.7019608  0.5294118  0.4627451\n",
      " 0.69803923 0.25490198 0.         0.01960784 0.         0.05882353\n",
      " 0.1882353  0.2509804  0.5254902  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.00392157 0.01568628 0.         0.         0.35686275 0.5137255\n",
      " 0.10588235 0.38039216 0.78431374 0.4117647  0.54509807 0.5254902\n",
      " 0.         0.         0.         0.3647059  0.7372549  0.7607843\n",
      " 0.2784314  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.01568628 0.         0.\n",
      " 0.07843138 0.3137255  0.5568628  0.4745098  0.09803922 0.5137255\n",
      " 0.627451   0.6745098  0.63529414 0.54901963 0.4862745  0.21176471\n",
      " 0.2509804  0.654902   0.6509804  0.69803923 0.09803922 0.\n",
      " 0.         0.         0.00392157 0.03137255 0.01960784 0.01960784\n",
      " 0.         0.         0.02745098 0.3372549  0.40784314 0.64705884\n",
      " 0.17254902 0.5921569  0.88235295 0.93333334 0.7176471  0.53333336\n",
      " 0.5686275  0.7019608  0.8039216  0.81960785 0.7647059  0.65882355\n",
      " 0.70980394 0.6901961  0.06666667 0.         0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.14901961\n",
      " 0.36078432 0.5764706  0.28235295 0.4745098  0.6156863  0.6156863\n",
      " 0.8509804  0.8235294  0.67058825 0.8039216  0.7254902  0.6901961\n",
      " 0.59607846 0.7176471  0.8        0.7607843  0.6745098  0.7607843\n",
      " 0.09803922 0.         0.         0.         0.         0.15294118\n",
      " 0.3372549  0.4745098  0.67058825 0.70980394 0.42745098 0.38039216\n",
      " 0.61960787 0.6509804  0.50980395 0.64705884 0.6313726  0.45490196\n",
      " 0.94509804 0.654902   0.5372549  0.8901961  0.61960787 0.5686275\n",
      " 0.8117647  0.78431374 0.6862745  0.8235294  0.26666668 0.\n",
      " 0.10588235 0.6        0.2        0.5254902  0.64705884 0.58431375\n",
      " 0.5058824  0.37254903 0.47058824 0.58431375 0.56078434 0.7529412\n",
      " 0.7647059  0.27058825 0.68235296 0.4392157  0.6392157  0.8901961\n",
      " 0.75686276 0.87058824 0.8509804  0.7882353  0.7921569  0.80784315\n",
      " 0.79607844 0.7921569  0.69411767 0.         0.34901962 0.64705884\n",
      " 0.5254902  0.25490198 0.14901961 0.3254902  0.40784314 0.57254905\n",
      " 0.5372549  0.6666667  0.4745098  0.23137255 0.8        0.7137255\n",
      " 0.44313726 1.         0.7921569  0.85882354 0.8156863  0.64705884\n",
      " 0.7019608  0.7294118  0.70980394 0.74509805 0.78431374 0.73333335\n",
      " 0.80784315 0.08235294 0.36862746 0.7019608  0.6392157  0.7294118\n",
      " 0.74509805 0.69803923 0.59607846 0.56078434 0.5568628  0.5764706\n",
      " 0.7294118  0.70980394 0.7647059  0.88235295 0.8784314  0.70980394\n",
      " 0.69411767 0.5921569  0.6392157  0.65882355 0.6313726  0.6392157\n",
      " 0.5921569  0.62352943 0.627451   0.70980394 0.69411767 0.\n",
      " 0.         0.6313726  0.69411767 0.5254902  0.63529414 0.6666667\n",
      " 0.7372549  0.7137255  0.6901961  0.6156863  0.654902   0.73333335\n",
      " 0.74509805 0.68235296 0.6313726  0.64705884 0.67058825 0.73333335\n",
      " 0.8039216  0.3764706  0.28627452 0.7490196  0.7254902  0.78431374\n",
      " 0.5137255  0.5137255  0.85882354 0.17254902 0.         0.\n",
      " 0.31764707 0.35686275 0.47058824 0.9647059  0.5882353  0.34509805\n",
      " 0.67058825 0.8509804  0.5058824  0.4        0.46666667 0.73333335\n",
      " 0.91764706 0.75686276 0.7607843  0.8117647  0.69411767 0.3764706\n",
      " 0.34117648 0.6392157  0.87058824 0.7294118  0.28627452 0.34509805\n",
      " 0.74509805 0.5058824  0.00392157 0.         0.         0.06666667\n",
      " 0.09019608 0.3254902  0.3019608  0.28627452 0.45490196 0.5568628\n",
      " 0.28627452 0.3137255  0.24313726 0.24705882 0.48235294 0.\n",
      " 0.         0.         0.15686275 0.28235295 0.21176471 0.43529412\n",
      " 0.4        0.19607843 0.30588236 0.28235295 0.2        0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ] [0 0 0 0 0 0 0 1 0 0]\n",
      "Epoch: 0, Loss: 2.200809038206961, Val Loss: 2.1378327724619\n",
      "Epoch: 1, Loss: 2.1370590138156675, Val Loss: 2.0789141061071215\n",
      "Epoch: 2, Loss: 2.0778871387525744, Val Loss: 2.0238834588788595\n",
      "Epoch: 3, Loss: 2.0226587588753304, Val Loss: 1.9723489779388756\n",
      "Epoch: 4, Loss: 1.9709561993618108, Val Loss: 1.9239498554406256\n",
      "Epoch: 5, Loss: 1.922372934327016, Val Loss: 1.8784072588113088\n",
      "Epoch: 6, Loss: 1.8766309646586918, Val Loss: 1.835456026085255\n",
      "Epoch: 7, Loss: 1.8334671309914037, Val Loss: 1.7948407814889134\n",
      "Epoch: 8, Loss: 1.7926489574465017, Val Loss: 1.7564061633899626\n",
      "Epoch: 9, Loss: 1.7540107133367564, Val Loss: 1.7199870931459373\n",
      "Epoch: 10, Loss: 1.7174025980350025, Val Loss: 1.685420954595146\n",
      "Epoch: 11, Loss: 1.6826654159325227, Val Loss: 1.6525992215366299\n",
      "Epoch: 12, Loss: 1.6496586692034176, Val Loss: 1.6213818631716495\n",
      "Epoch: 13, Loss: 1.6182619391790867, Val Loss: 1.5916585751955905\n",
      "Epoch: 14, Loss: 1.5883688319626226, Val Loss: 1.5633319863401827\n",
      "Epoch: 15, Loss: 1.559893158096077, Val Loss: 1.536314480714044\n",
      "Epoch: 16, Loss: 1.5327385263850533, Val Loss: 1.51052177377294\n",
      "Epoch: 17, Loss: 1.5068135010805321, Val Loss: 1.4858721335219742\n",
      "Epoch: 18, Loss: 1.4820372270179718, Val Loss: 1.4622892531781357\n",
      "Epoch: 19, Loss: 1.4583482366085334, Val Loss: 1.4397122431144787\n",
      "Epoch: 20, Loss: 1.4356780279901855, Val Loss: 1.4180790769071396\n",
      "Epoch: 21, Loss: 1.4139520644158914, Val Loss: 1.397356966389941\n",
      "Epoch: 22, Loss: 1.3931364617957014, Val Loss: 1.3774914464702137\n",
      "Epoch: 23, Loss: 1.3731759323912303, Val Loss: 1.3584256286290672\n",
      "Epoch: 24, Loss: 1.3540251497897364, Val Loss: 1.3401341222866785\n",
      "Epoch: 25, Loss: 1.3356568631985188, Val Loss: 1.3225678645815766\n",
      "Epoch: 26, Loss: 1.3180178763578512, Val Loss: 1.305688307714871\n",
      "Epoch: 27, Loss: 1.3010750192312261, Val Loss: 1.2894618134413915\n",
      "Epoch: 28, Loss: 1.2847904808763713, Val Loss: 1.2738619698704376\n",
      "Epoch: 29, Loss: 1.2691401581476964, Val Loss: 1.2588502768539183\n",
      "Epoch: 30, Loss: 1.254086239841011, Val Loss: 1.2443987178475653\n",
      "Epoch: 31, Loss: 1.239599223389908, Val Loss: 1.2304881635989597\n",
      "Epoch: 32, Loss: 1.225656383944804, Val Loss: 1.2170987739748411\n",
      "Epoch: 33, Loss: 1.2122318343323637, Val Loss: 1.2042032834108427\n",
      "Epoch: 34, Loss: 1.1993006274558387, Val Loss: 1.1917731361477193\n",
      "Epoch: 35, Loss: 1.1868357062061226, Val Loss: 1.1797924567125158\n",
      "Epoch: 36, Loss: 1.1748185903599084, Val Loss: 1.1682379626111405\n",
      "Epoch: 37, Loss: 1.1632301413271242, Val Loss: 1.1570917620354142\n",
      "Epoch: 38, Loss: 1.1520489591000458, Val Loss: 1.1463315442061737\n",
      "Epoch: 39, Loss: 1.1412559307567467, Val Loss: 1.135940210826601\n",
      "Epoch: 40, Loss: 1.130834304651027, Val Loss: 1.1258974601242953\n",
      "Epoch: 41, Loss: 1.120764237677025, Val Loss: 1.1161871237307102\n",
      "Epoch: 42, Loss: 1.1110300750781867, Val Loss: 1.1067978667433986\n",
      "Epoch: 43, Loss: 1.1016175250928393, Val Loss: 1.0977126324081232\n",
      "Epoch: 44, Loss: 1.0925100013160376, Val Loss: 1.0889199423054454\n",
      "Epoch: 45, Loss: 1.0836935446694753, Val Loss: 1.0804035053119607\n",
      "Epoch: 46, Loss: 1.0751546384062443, Val Loss: 1.0721510029557033\n",
      "Epoch: 47, Loss: 1.0668817097705978, Val Loss: 1.0641499846758042\n",
      "Epoch: 48, Loss: 1.0588605667988724, Val Loss: 1.056388313347444\n",
      "Epoch: 49, Loss: 1.0510793513270509, Val Loss: 1.048858350438101\n",
      "Epoch: 50, Loss: 1.0435298925178096, Val Loss: 1.0415504005072809\n",
      "Epoch: 51, Loss: 1.0362021038146525, Val Loss: 1.0344540775395261\n",
      "Epoch: 52, Loss: 1.0290846567513352, Val Loss: 1.0275599969505902\n",
      "Epoch: 53, Loss: 1.022168219127715, Val Loss: 1.0208583838260061\n",
      "Epoch: 54, Loss: 1.0154434087326858, Val Loss: 1.014341725475011\n",
      "Epoch: 55, Loss: 1.008903800307805, Val Loss: 1.0080049198183023\n",
      "Epoch: 56, Loss: 1.002542624354605, Val Loss: 1.0018375872400582\n",
      "Epoch: 57, Loss: 0.996351890682791, Val Loss: 0.995830001979945\n",
      "Epoch: 58, Loss: 0.9903226789333137, Val Loss: 0.989977238646331\n",
      "Epoch: 59, Loss: 0.9844491465196655, Val Loss: 0.984272613328927\n",
      "Epoch: 60, Loss: 0.9787247654900854, Val Loss: 0.9787128735637226\n",
      "Epoch: 61, Loss: 0.973145624254526, Val Loss: 0.9732908163988222\n",
      "Epoch: 62, Loss: 0.9677044402035795, Val Loss: 0.968001561816323\n",
      "Epoch: 63, Loss: 0.9623959296526667, Val Loss: 0.9628401021818704\n",
      "Epoch: 64, Loss: 0.9572139166962881, Val Loss: 0.9578013601686941\n",
      "Epoch: 65, Loss: 0.9521553331157736, Val Loss: 0.9528799575706144\n",
      "Epoch: 66, Loss: 0.9472144491097154, Val Loss: 0.948072952864604\n",
      "Epoch: 67, Loss: 0.9423878703415877, Val Loss: 0.9433756219973051\n",
      "Epoch: 68, Loss: 0.9376713158473999, Val Loss: 0.9387843623002479\n",
      "Epoch: 69, Loss: 0.9330608553974018, Val Loss: 0.9342944513090607\n",
      "Epoch: 70, Loss: 0.9285525470064991, Val Loss: 0.9299009654262992\n",
      "Epoch: 71, Loss: 0.9241428768964851, Val Loss: 0.925601965764711\n",
      "Epoch: 72, Loss: 0.9198288159078849, Val Loss: 0.9213945366288672\n",
      "Epoch: 73, Loss: 0.9156065637631057, Val Loss: 0.917276349845054\n",
      "Epoch: 74, Loss: 0.911472968124157, Val Loss: 0.9132433886376514\n",
      "Epoch: 75, Loss: 0.9074247468990112, Val Loss: 0.9092926293802486\n",
      "Epoch: 76, Loss: 0.9034591955789483, Val Loss: 0.9054214520177738\n",
      "Epoch: 77, Loss: 0.8995728544693324, Val Loss: 0.9016273790921092\n",
      "Epoch: 78, Loss: 0.8957637153034245, Val Loss: 0.8979078388304553\n",
      "Epoch: 79, Loss: 0.8920291441699252, Val Loss: 0.8942588154993649\n",
      "Epoch: 80, Loss: 0.8883672070065963, Val Loss: 0.8906782714469652\n",
      "Epoch: 81, Loss: 0.8847749950596904, Val Loss: 0.8871660562691305\n",
      "Epoch: 82, Loss: 0.8812509678213419, Val Loss: 0.8837188327681658\n",
      "Epoch: 83, Loss: 0.8777928643936843, Val Loss: 0.8803345927521645\n",
      "Epoch: 84, Loss: 0.8743974361716828, Val Loss: 0.8770113149807797\n",
      "Epoch: 85, Loss: 0.8710639118783753, Val Loss: 0.8737477799609162\n",
      "Epoch: 86, Loss: 0.8677899401948496, Val Loss: 0.8705421885830383\n",
      "Epoch: 87, Loss: 0.8645732332854428, Val Loss: 0.8673928849542284\n",
      "Epoch: 88, Loss: 0.8614130277310585, Val Loss: 0.8642987758428614\n",
      "Epoch: 89, Loss: 0.8583079094950264, Val Loss: 0.8612578574538647\n",
      "Epoch: 90, Loss: 0.8552559365859488, Val Loss: 0.8582682880875296\n",
      "Epoch: 91, Loss: 0.8522552188404913, Val Loss: 0.8553291799959273\n",
      "Epoch: 92, Loss: 0.8493045612618645, Val Loss: 0.852439178280706\n",
      "Epoch: 93, Loss: 0.8464030402519856, Val Loss: 0.8495969313712625\n",
      "Epoch: 94, Loss: 0.8435490099574028, Val Loss: 0.8468002903621558\n",
      "Epoch: 95, Loss: 0.8407410151574184, Val Loss: 0.8440478026200353\n",
      "Epoch: 96, Loss: 0.8379782261750965, Val Loss: 0.8413394958593304\n",
      "Epoch: 97, Loss: 0.8352600976470597, Val Loss: 0.8386723964518845\n",
      "Epoch: 98, Loss: 0.8325841183140122, Val Loss: 0.8360467647304108\n",
      "Epoch: 99, Loss: 0.8299496070888076, Val Loss: 0.8334616529384355\n",
      "Epoch: 100, Loss: 0.827355230870166, Val Loss: 0.8309158043658947\n",
      "Epoch: 101, Loss: 0.8247998895676228, Val Loss: 0.8284078908335438\n",
      "Epoch: 102, Loss: 0.8222827199766646, Val Loss: 0.8259371740935796\n",
      "Epoch: 103, Loss: 0.8198030390916801, Val Loss: 0.8235032389192981\n",
      "Epoch: 104, Loss: 0.817360396850963, Val Loss: 0.8211045672735044\n",
      "Epoch: 105, Loss: 0.8149531311862515, Val Loss: 0.8187409474055041\n",
      "Epoch: 106, Loss: 0.8125809337156118, Val Loss: 0.8164099556767385\n",
      "Epoch: 107, Loss: 0.8102419720943017, Val Loss: 0.8141119698724935\n",
      "Epoch: 108, Loss: 0.8079365426646536, Val Loss: 0.8118465109185332\n",
      "Epoch: 109, Loss: 0.8056633834582856, Val Loss: 0.8096123612583763\n",
      "Epoch: 110, Loss: 0.8034217457251874, Val Loss: 0.8074085992014643\n",
      "Epoch: 111, Loss: 0.8012109175980671, Val Loss: 0.8052344313707174\n",
      "Epoch: 112, Loss: 0.7990302096444309, Val Loss: 0.8030893569261794\n",
      "Epoch: 113, Loss: 0.796879046393927, Val Loss: 0.8009733189738466\n",
      "Epoch: 114, Loss: 0.794756648357735, Val Loss: 0.7988851290272925\n",
      "Epoch: 115, Loss: 0.7926621993066985, Val Loss: 0.7968247274275739\n",
      "Epoch: 116, Loss: 0.7905951829173141, Val Loss: 0.7947907741675887\n",
      "Epoch: 117, Loss: 0.7885547181691966, Val Loss: 0.7927831686638944\n",
      "Epoch: 118, Loss: 0.7865406341408591, Val Loss: 0.7908011746894854\n",
      "Epoch: 119, Loss: 0.7845518970667446, Val Loss: 0.788844200705728\n",
      "Epoch: 120, Loss: 0.7825883912310947, Val Loss: 0.7869119101778389\n",
      "Epoch: 121, Loss: 0.7806498658336439, Val Loss: 0.7850035499199959\n",
      "Epoch: 122, Loss: 0.7787354211113993, Val Loss: 0.7831193585186657\n",
      "Epoch: 123, Loss: 0.7768449928721863, Val Loss: 0.7812589347077007\n",
      "Epoch: 124, Loss: 0.7749779910201302, Val Loss: 0.7794205898672857\n",
      "Epoch: 125, Loss: 0.7731334582117779, Val Loss: 0.7776045362151277\n",
      "Epoch: 126, Loss: 0.7713113244765276, Val Loss: 0.7758095265150241\n",
      "Epoch: 127, Loss: 0.7695106401318421, Val Loss: 0.7740355571415029\n",
      "Epoch: 128, Loss: 0.767731435541715, Val Loss: 0.7722825887539592\n",
      "Epoch: 129, Loss: 0.7659736503896231, Val Loss: 0.7705504484892151\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7511666666666666\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, numpy as np, matplotlib.pyplot as plt\n",
    "folder = \"/kaggle/input/fashion-mnist/final\" # folder containing the data\n",
    "\n",
    "def load_data(X, y):\n",
    "    for f in os.listdir(folder):\n",
    "        for file in os.listdir(f\"{folder}/{f}\"):\n",
    "            img = plt.imread(f\"{folder}/{f}/{file}\")\n",
    "            X.append(img)\n",
    "\n",
    "            # The most obvious choice for the label is the class (folder) name\n",
    "            # label = int(f)\n",
    "\n",
    "            # [Q1] But we dont use it here why? Why is it an array of 10 elements?\n",
    "            # Clue: Lookup one hot encoding\n",
    "            # Read up on Cross Entropy Loss\n",
    "\n",
    "            label = [0] * 10\n",
    "            label[int(f)] = 1 # Why is this array the label and not a numeber?\n",
    "\n",
    "            y.append(label)\n",
    "            \n",
    "        print(f\"Loaded {f} class\")\n",
    "\n",
    "X, y = [], []\n",
    "load_data(X, y)\n",
    "\n",
    "# [Q2] Why convert to numpy array?\n",
    "X=np.array(X)\n",
    "y=np.array(y)\n",
    "\n",
    "X = X[:, :,:, 0] # [Q3] Why are we doing this and what does this type of slicing result in?\n",
    "X = X.reshape(X.shape[0], X.shape[1]*X.shape[2]) # [Q4] Why are we reshaping the data?\n",
    "print(\"After reshaping\")\n",
    "print(X.shape, y.shape)\n",
    "print(X[0], y[0])\n",
    "\n",
    "class NN:\n",
    "    def __init__(self, input_neurons, hidden_neurons, output_neurons, learning_rate, epochs):\n",
    "        \"\"\"\n",
    "        Class Definition\n",
    "        \n",
    "        We use a class because it is easy to visualize the process of training a neural network\n",
    "        It's also easier to resuse and repurpose depending on the task at hand\n",
    "\n",
    "        We have a simple neural network, with an input layer, one hidden (middle) layer and an output layer\n",
    "\n",
    "        input_neurons: Number of neurons in the input layer\n",
    "        hidden_neurons: Number of neurons in the hidden layer\n",
    "        output_neurons: Number of neurons in the output layer\n",
    "        learning_rate: The rate at which the weights are updated [Q5] What is the learning rate?\n",
    "        epochs: Number of times the model will train on the entire dataset \n",
    "        \"\"\"\n",
    "\n",
    "        self.input_neurons = input_neurons\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.output_neurons = output_neurons\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        \"\"\"\n",
    "        Weights and Biases\n",
    "\n",
    "        At this point you should know what weights and biases are in a neural network and if not, go check out the 3blue1brown video on Neural Networks\n",
    "        What matters here is however the matrix dimensions of the weights and biases\n",
    "\n",
    "         [Q6] Why are the dimensions of the weights and biases the way they are? \n",
    "        \n",
    "        Try to figure out the dimensions of the weights and biases for the hidden and output layers\n",
    "        Try to see what equations represent the forward pass (basically the prediction)\n",
    "        And then, try to see if the dimensions of the matrix multiplications are correct\n",
    "\n",
    "        Note: The bias dimensions may not match. Look up broadcasting in numpy to understand\n",
    "        [Q7] What is broadcasting and why do we need to broadcast the bias?\n",
    "        \"\"\"\n",
    "\n",
    "        # Ideally any random set of weights and biases can be used to initialize the network\n",
    "        # self.wih = np.random.randn(hidden_neurons, input_neurons)\n",
    "\n",
    "        # [Q8] What is np.random.randn? What's the shape of this matrix?\n",
    "\n",
    "        # Optional: Try to figure out why the weights are initialized this way\n",
    "        # Note: You can just use the commented out line above if you don't want to do this\n",
    "\n",
    "        self.wih = np.random.randn(hidden_neurons, input_neurons) * np.sqrt(2/input_neurons)\n",
    "        self.bih = np.zeros((hidden_neurons, 1))\n",
    "\n",
    "        self.who = np.random.randn(output_neurons, hidden_neurons) * np.sqrt(2/hidden_neurons)\n",
    "        self.bho = np.zeros((output_neurons, 1))\n",
    "\n",
    "    # Activation Functions and their derivatives\n",
    "    # [Q9] What are activation functions and why do we need them?\n",
    "\n",
    "    def relu(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the RELU function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return z * (z > 0)\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the Sigmoid function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def relu_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the RELU derivative function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return 1 * (z > 0)\n",
    "\n",
    "    def sigmoid_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the Sigmoid derivative function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return z * (1 - z)\n",
    "\n",
    "    # [Q10] What is the softmax function and why do we need it? Read up on it\n",
    "    def softmax(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the Softmax function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "\n",
    "    def softmax_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the Softmax derivative function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return z * (1 - z)\n",
    "\n",
    "    # Loss Functions and their derivatives\n",
    "    # [Q11] What are loss functions and why do we need them?\n",
    "\n",
    "    def mean_squared_error(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implementation of the Mean Squared Error function\n",
    "        y: (10, n)\n",
    "        y_hat: (10, n)\n",
    "        returns (1, n)\n",
    "        \"\"\"\n",
    "        return np.mean((y - y_hat) ** 2, axis=0)\n",
    "\n",
    "    def cross_entropy_loss(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implementation of the Cross Entropy Loss function\n",
    "        y: (10, n)\n",
    "        y_hat: (10, n)\n",
    "        returns (1, n)\n",
    "        \"\"\"\n",
    "\n",
    "        # Implement the cross entropy loss function here and return it\n",
    "        # Keep the dimensions of the input in mind when writing the code\n",
    "\n",
    "        # [Code Goes Here]\n",
    "        logits, batch_size = y.shape\n",
    "        loss = []\n",
    "        epsilon = 1e-9  # I got runtime error which I debugged and found out this log(0) problem so had to declare this using help of internet\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            t = 0\n",
    "            for j in range(logits):\n",
    "                t += -y[j, i] * np.log(y_hat[j, i] + epsilon)\n",
    "            loss.append(t)\n",
    "        loss = np.array([loss])\n",
    "        return loss\n",
    "        \n",
    "    def mean_squared_error_derivative(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implementation of the Mean Squared Error derivative function\n",
    "        y: (10, n)\n",
    "        y_hat: (10, n)\n",
    "        returns (10, n)\n",
    "        \"\"\"\n",
    "        return y_hat - y\n",
    "\n",
    "    def cross_entropy_derivative(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implementation of the Cross Entropy Loss derivative function\n",
    "        y: (10, n)\n",
    "        y_hat: (10, n)\n",
    "        returns (10, n)\n",
    "        \"\"\"\n",
    "        return y_hat - y\n",
    "        # Implement the cross entropy loss derivative function here and return it\n",
    "        # Note: The derivative of the CEL is usually taken with respect to the softmax input not output so keep that in mind while writing\n",
    "\n",
    "        # [Code Goes Here]\n",
    "        \n",
    "    # Forward propagation\n",
    "    def forward(self, input_list):\n",
    "        \"\"\"\n",
    "        Implementation of the Forward Pass\n",
    "        input_list: (784, n)        - n is the number of images\n",
    "        returns (10, n)              - n is the number of images\n",
    "    \n",
    "        Now we come to the heart of the neural network, the forward pass\n",
    "        This is where the input is passed through the network to get the output\n",
    "\n",
    "        [Q12] What does the output choice we have here mean? It's an array of 10 elements per image, but why?\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = np.array(input_list, ndmin=2).T\n",
    "        inputs = inputs - np.mean(inputs) # [Q13] Why are we subtracting the mean of the inputs?\n",
    "        \n",
    "        # To get to the hidden layer:\n",
    "        # Multiply the input with the weights and adding the bias\n",
    "        # Apply the activation function (relu in this case)\n",
    "\n",
    "        # [Code Goes Here]\n",
    "        z_hidden = np.dot(self.wih, inputs) + self.bih\n",
    "        a_hidden = self.relu(z_hidden)\n",
    "\n",
    "\n",
    "        # To get to the output layer:\n",
    "        # Multiply the hidden layer output with the weights and adding the bias\n",
    "        # Apply the activation function (softmax in this case)\n",
    "        # [Q14] Why are we using the softmax function here?\n",
    "\n",
    "        # [Code Goes Here]\n",
    "        z_output = np.dot(self.who, a_hidden) + self.bho\n",
    "        y_hat = self.softmax(z_output)\n",
    "\n",
    "\n",
    "        # Return it\n",
    "\n",
    "        # [Code Goes Here]\n",
    "        return y_hat\n",
    "        \n",
    "\n",
    "    # Back propagation\n",
    "    def backprop(self, inputs_list, targets_list):\n",
    "        \"\"\"\n",
    "        Implementation of the Backward Pass\n",
    "        inputs_list: (784, n)\n",
    "        targets_list: (10, n)\n",
    "        returns a scalar value (loss)\n",
    "        \n",
    "        This is where the magic happens, the backpropagation algorithm\n",
    "        This is where the weights are updated based on the error in the prediction of the network\n",
    "\n",
    "        Now, the calculus involved is fairly complicated, especially because it's being done in matrix form\n",
    "        However the intuition is simple. \n",
    "\n",
    "        Since this is a recruitment stage, most of the function is written out for you, so follow along with the comments\n",
    "        \"\"\"\n",
    "\n",
    "        # Basic forward pass to get the outputs\n",
    "        # Obviously we need the predictions to know how the model is doing\n",
    "        # [Q15] Why are we doing a forward pass here instead of just using the outputs from the forward function?\n",
    "        # Is there any actual reason, or could we just swap it?\n",
    "\n",
    "        inputs = np.array(inputs_list, ndmin=2).T # (784, n)\n",
    "        inputs = inputs - np.mean(inputs)\n",
    "\n",
    "        tj = np.array(targets_list, ndmin=2).T # (10, n)\n",
    "\n",
    "        hidden_inputs = np.dot(self.wih, inputs) + self.bih\n",
    "        hidden_outputs = self.relu(hidden_inputs)\n",
    "\n",
    "        final_inputs = np.dot(self.who, hidden_outputs) + self.bho\n",
    "        yj = self.softmax(final_inputs)\n",
    "\n",
    "        # Calculating the loss - This is the error in the prediction\n",
    "        # The loss then is the indication of how well the model is doing, its a useful parameter to track to see if the model is improving\n",
    "\n",
    "        loss = self.cross_entropy_loss(tj, yj) # Convert this to cross entropy loss\n",
    "\n",
    "\n",
    "        # Updating the weights using Update Rule\n",
    "        # Now that we have the incorrect predictions, we can update the weights to make the predictions better\n",
    "        # This is done using the gradient of the loss function with respect to the weights\n",
    "        # Basically, we know how much the overall error is caused due to individual weights using the chain rule of calculus\n",
    "        # Since we want to minimise the error, we move in the opposite direction of something like a \"derivative\" of the error with respect to the weights\n",
    "        # Calculus therefore helps us find the direction in which we should move to reduce the error\n",
    "        # A direction means what delta W changes we need to make to make the model better\n",
    "\n",
    "        # Output Layer - We start with the output layer because we are backtracking how the error is caused\n",
    "        # Think of it as using the derivatives of each layer while going back\n",
    "\n",
    "\n",
    "        # For the task, you will be using Cross Entropy Loss\n",
    "\n",
    "        # Change this to cross entropy loss\n",
    "        dE_dzo = self.cross_entropy_derivative(tj, yj) # (10,n)\n",
    "        # Note: the derivative of the CEL is usually taken with respect to the softmax input not output so keep that in mind while writing\n",
    "\n",
    "\n",
    "        dE_dwho = np.dot(dE_dzo, hidden_outputs.T) / hidden_outputs.shape[1] # dot((10,n) (n,128) = (10,128)\n",
    "        dE_dbho = np.mean(dE_dzo, axis=1, keepdims=True) # sum((10,n), axis=1) = (10,1)\n",
    "        \n",
    "        self.who -= self.lr * dE_dwho\n",
    "        self.bho -= self.lr * dE_dbho\n",
    "\n",
    "        # Hidden Layer\n",
    "        dE_dah = np.dot(self.who.T, dE_dzo) # dot((128,10), (10,n)) = (128,n)\n",
    "        dE_dzh = dE_dah * self.relu_derivative(hidden_inputs)\n",
    "        dE_dwih = np.dot(dE_dzh, inputs.T) / inputs.shape[1]\n",
    "        dE_dbih = np.mean(dE_dzh, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "        self.wih -= self.lr * dE_dwih\n",
    "        self.bih -= self.lr * dE_dbih\n",
    "\n",
    "        return np.mean(loss)\n",
    "\n",
    "    def fit(self, inputs_list, targets_list,validation_data, validation_labels):\n",
    "        \"\"\"\n",
    "        Implementation of the training loop\n",
    "        inputs_list: (784, n)\n",
    "        targets_list: (10, n)\n",
    "        validation_data: (784, n)\n",
    "        validation_labels: (10, n)\n",
    "        returns train_loss, val_loss\n",
    "\n",
    "        This is where the training loop is implemented\n",
    "        We loop over the entire dataset for a certain number of epochs\n",
    "        We also track the validation loss to see how well the model is generalizing\n",
    "        [Q16] What is the validation dataset and what do we mean by generalization?\n",
    "\n",
    "        We also return the training and validation loss to see how the model is improving\n",
    "        It's a good idea to plot these to see how the model is doing\n",
    "        \"\"\"\n",
    "\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "        for epoch in range(self.epochs):\n",
    "            loss = self.backprop(inputs_list, targets_list)\n",
    "            train_loss.append(loss)\n",
    "            vloss = self.cross_entropy_loss(validation_labels.T, self.forward(validation_data))\n",
    "            val_loss.append(np.mean(vloss)) \n",
    "            print(f\"Epoch: {epoch}, Loss: {loss}, Val Loss: {val_loss[-1]}\")\n",
    "\n",
    "        return train_loss[1:], val_loss[:-1] \n",
    "\n",
    "    def predict(self, X):\n",
    "        outputs = self.forward(X).T\n",
    "        return outputs\n",
    "\n",
    "# This is where the class is used to train the model\n",
    "\n",
    "# The parameters in the model are (input_neurons, hidden_neurons, output_neurons, learning_rate, epochs)\n",
    "# These parameters aren't the right parameters, so tweak them to get the best results\n",
    "# Around 70% accuracy is a good end goal (75% is great) but for the recruitment task, 60% is good enough\n",
    "\n",
    "# [Q17] What are the parameters in the model and what do they mean?\n",
    "\n",
    "fashion_mnist = NN(784, 160, 10, 0.03, 130)\n",
    "p = np.random.permutation(len(X))\n",
    "X, y = X[p], y[p]\n",
    "\n",
    "# Splitting the data into training, validation and testing in the ratio 70:20:10\n",
    "X_train, y_train = X[:int(0.7*len(X))], y[:int(0.7*len(X))]\n",
    "X_val, y_val = X[int(0.7*len(X)):int(0.9*len(X))], y[int(0.7*len(X)):int(0.9*len(X))]\n",
    "X_test, y_test = X[int(0.9*len(X)):], y[int(0.9*len(X)):]\n",
    "\n",
    "# Training the model\n",
    "train_loss,val_loss = fashion_mnist.fit(X_train, y_train,X_val,y_val)\n",
    "\n",
    "\n",
    "# Plotting the loss\n",
    "plt.plot(train_loss,label='train')\n",
    "plt.plot(val_loss,label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred = fashion_mnist.predict(X_test)\n",
    "\n",
    "# [Q18] Why are we using argmax here? Why is this output different from the output of the model?\n",
    "y_pred = np.argmax(y_pred, axis=1)  \n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "print(f\"Accuracy: {np.mean(y_pred == y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8317357,
     "sourceId": 13129288,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 377.502095,
   "end_time": "2025-09-22T16:50:50.442872",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-22T16:44:32.940777",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
