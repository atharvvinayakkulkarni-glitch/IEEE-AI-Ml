{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46ea220a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-21T19:43:17.438457Z",
     "iopub.status.busy": "2025-09-21T19:43:17.438079Z",
     "iopub.status.idle": "2025-09-21T19:50:57.459106Z",
     "shell.execute_reply": "2025-09-21T19:50:57.457907Z"
    },
    "papermill": {
     "duration": 460.026831,
     "end_time": "2025-09-21T19:50:57.460737",
     "exception": false,
     "start_time": "2025-09-21T19:43:17.433906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7 class\n",
      "Loaded 2 class\n",
      "Loaded 5 class\n",
      "Loaded 8 class\n",
      "Loaded 0 class\n",
      "Loaded 3 class\n",
      "Loaded 1 class\n",
      "Loaded 4 class\n",
      "Loaded 9 class\n",
      "Loaded 6 class\n",
      "After reshaping\n",
      "(60000, 784) (60000, 10)\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00392157 0.         0.         0.1254902  0.1882353\n",
      " 0.         0.52156866 0.28627452 0.         0.         0.\n",
      " 0.         0.         0.         0.01568628 0.1254902  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.01960784 0.\n",
      " 0.00392157 0.15686275 0.5254902  0.7019608  0.5294118  0.4627451\n",
      " 0.69803923 0.25490198 0.         0.01960784 0.         0.05882353\n",
      " 0.1882353  0.2509804  0.5254902  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.00392157 0.01568628 0.         0.         0.35686275 0.5137255\n",
      " 0.10588235 0.38039216 0.78431374 0.4117647  0.54509807 0.5254902\n",
      " 0.         0.         0.         0.3647059  0.7372549  0.7607843\n",
      " 0.2784314  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.01568628 0.         0.\n",
      " 0.07843138 0.3137255  0.5568628  0.4745098  0.09803922 0.5137255\n",
      " 0.627451   0.6745098  0.63529414 0.54901963 0.4862745  0.21176471\n",
      " 0.2509804  0.654902   0.6509804  0.69803923 0.09803922 0.\n",
      " 0.         0.         0.00392157 0.03137255 0.01960784 0.01960784\n",
      " 0.         0.         0.02745098 0.3372549  0.40784314 0.64705884\n",
      " 0.17254902 0.5921569  0.88235295 0.93333334 0.7176471  0.53333336\n",
      " 0.5686275  0.7019608  0.8039216  0.81960785 0.7647059  0.65882355\n",
      " 0.70980394 0.6901961  0.06666667 0.         0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.14901961\n",
      " 0.36078432 0.5764706  0.28235295 0.4745098  0.6156863  0.6156863\n",
      " 0.8509804  0.8235294  0.67058825 0.8039216  0.7254902  0.6901961\n",
      " 0.59607846 0.7176471  0.8        0.7607843  0.6745098  0.7607843\n",
      " 0.09803922 0.         0.         0.         0.         0.15294118\n",
      " 0.3372549  0.4745098  0.67058825 0.70980394 0.42745098 0.38039216\n",
      " 0.61960787 0.6509804  0.50980395 0.64705884 0.6313726  0.45490196\n",
      " 0.94509804 0.654902   0.5372549  0.8901961  0.61960787 0.5686275\n",
      " 0.8117647  0.78431374 0.6862745  0.8235294  0.26666668 0.\n",
      " 0.10588235 0.6        0.2        0.5254902  0.64705884 0.58431375\n",
      " 0.5058824  0.37254903 0.47058824 0.58431375 0.56078434 0.7529412\n",
      " 0.7647059  0.27058825 0.68235296 0.4392157  0.6392157  0.8901961\n",
      " 0.75686276 0.87058824 0.8509804  0.7882353  0.7921569  0.80784315\n",
      " 0.79607844 0.7921569  0.69411767 0.         0.34901962 0.64705884\n",
      " 0.5254902  0.25490198 0.14901961 0.3254902  0.40784314 0.57254905\n",
      " 0.5372549  0.6666667  0.4745098  0.23137255 0.8        0.7137255\n",
      " 0.44313726 1.         0.7921569  0.85882354 0.8156863  0.64705884\n",
      " 0.7019608  0.7294118  0.70980394 0.74509805 0.78431374 0.73333335\n",
      " 0.80784315 0.08235294 0.36862746 0.7019608  0.6392157  0.7294118\n",
      " 0.74509805 0.69803923 0.59607846 0.56078434 0.5568628  0.5764706\n",
      " 0.7294118  0.70980394 0.7647059  0.88235295 0.8784314  0.70980394\n",
      " 0.69411767 0.5921569  0.6392157  0.65882355 0.6313726  0.6392157\n",
      " 0.5921569  0.62352943 0.627451   0.70980394 0.69411767 0.\n",
      " 0.         0.6313726  0.69411767 0.5254902  0.63529414 0.6666667\n",
      " 0.7372549  0.7137255  0.6901961  0.6156863  0.654902   0.73333335\n",
      " 0.74509805 0.68235296 0.6313726  0.64705884 0.67058825 0.73333335\n",
      " 0.8039216  0.3764706  0.28627452 0.7490196  0.7254902  0.78431374\n",
      " 0.5137255  0.5137255  0.85882354 0.17254902 0.         0.\n",
      " 0.31764707 0.35686275 0.47058824 0.9647059  0.5882353  0.34509805\n",
      " 0.67058825 0.8509804  0.5058824  0.4        0.46666667 0.73333335\n",
      " 0.91764706 0.75686276 0.7607843  0.8117647  0.69411767 0.3764706\n",
      " 0.34117648 0.6392157  0.87058824 0.7294118  0.28627452 0.34509805\n",
      " 0.74509805 0.5058824  0.00392157 0.         0.         0.06666667\n",
      " 0.09019608 0.3254902  0.3019608  0.28627452 0.45490196 0.5568628\n",
      " 0.28627452 0.3137255  0.24313726 0.24705882 0.48235294 0.\n",
      " 0.         0.         0.15686275 0.28235295 0.21176471 0.43529412\n",
      " 0.4        0.19607843 0.30588236 0.28235295 0.2        0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ] [0 0 0 0 0 0 0 1 0 0]\n",
      "Epoch: 0, Loss: 2.352290614447934, Val Loss: 0.09141602002163544\n",
      "Epoch: 1, Loss: 2.3447849937146743, Val Loss: 0.09123648999670372\n",
      "Epoch: 2, Loss: 2.3373907601089727, Val Loss: 0.09106078967058635\n",
      "Epoch: 3, Loss: 2.330100356769154, Val Loss: 0.09088849246755744\n",
      "Epoch: 4, Loss: 2.3229047037194257, Val Loss: 0.09071925357609116\n",
      "Epoch: 5, Loss: 2.3157965022620863, Val Loss: 0.09055277144058016\n",
      "Epoch: 6, Loss: 2.308768963635603, Val Loss: 0.09038874536023164\n",
      "Epoch: 7, Loss: 2.3018160057052777, Val Loss: 0.09022692546895492\n",
      "Epoch: 8, Loss: 2.2949308331149747, Val Loss: 0.09006711145934192\n",
      "Epoch: 9, Loss: 2.288108588843713, Val Loss: 0.08990905317191268\n",
      "Epoch: 10, Loss: 2.2813435898728804, Val Loss: 0.08975258212120044\n",
      "Epoch: 11, Loss: 2.2746323787837057, Val Loss: 0.08959753891704848\n",
      "Epoch: 12, Loss: 2.2679713183032004, Val Loss: 0.08944377822567921\n",
      "Epoch: 13, Loss: 2.2613580342191533, Val Loss: 0.08929116621051439\n",
      "Epoch: 14, Loss: 2.2547890755802165, Val Loss: 0.08913954802893144\n",
      "Epoch: 15, Loss: 2.248260461341262, Val Loss: 0.08898882268218135\n",
      "Epoch: 16, Loss: 2.2417706285962957, Val Loss: 0.08883886366720756\n",
      "Epoch: 17, Loss: 2.2353170185223243, Val Loss: 0.08868956039257828\n",
      "Epoch: 18, Loss: 2.228897417774217, Val Loss: 0.08854084134727078\n",
      "Epoch: 19, Loss: 2.2225091881122205, Val Loss: 0.08839263245293938\n",
      "Epoch: 20, Loss: 2.2161511640187697, Val Loss: 0.08824485888555504\n",
      "Epoch: 21, Loss: 2.209821045722829, Val Loss: 0.08809741094953892\n",
      "Epoch: 22, Loss: 2.2035172079331105, Val Loss: 0.08795019926571121\n",
      "Epoch: 23, Loss: 2.1972372332025705, Val Loss: 0.08780314826962418\n",
      "Epoch: 24, Loss: 2.1909796538566932, Val Loss: 0.08765619861672364\n",
      "Epoch: 25, Loss: 2.184741023208823, Val Loss: 0.08750932980813146\n",
      "Epoch: 26, Loss: 2.1785227727680287, Val Loss: 0.08736247971648839\n",
      "Epoch: 27, Loss: 2.1723224651784694, Val Loss: 0.08721560180333211\n",
      "Epoch: 28, Loss: 2.1661393909157205, Val Loss: 0.087068632063406\n",
      "Epoch: 29, Loss: 2.15997328660705, Val Loss: 0.08692153407899358\n",
      "Epoch: 30, Loss: 2.153823190750724, Val Loss: 0.0867742996616547\n",
      "Epoch: 31, Loss: 2.147688301661113, Val Loss: 0.08662687926707288\n",
      "Epoch: 32, Loss: 2.1415673622170295, Val Loss: 0.08647922548750273\n",
      "Epoch: 33, Loss: 2.1354597733476446, Val Loss: 0.08633131508050171\n",
      "Epoch: 34, Loss: 2.1293647063179577, Val Loss: 0.08618312144146759\n",
      "Epoch: 35, Loss: 2.1232818313297557, Val Loss: 0.08603455463308332\n",
      "Epoch: 36, Loss: 2.1172096218501144, Val Loss: 0.08588558870490962\n",
      "Epoch: 37, Loss: 2.1111465277208143, Val Loss: 0.08573621520983189\n",
      "Epoch: 38, Loss: 2.105093627275424, Val Loss: 0.08558637828511174\n",
      "Epoch: 39, Loss: 2.0990500991749257, Val Loss: 0.08543608604973925\n",
      "Epoch: 40, Loss: 2.0930159422657995, Val Loss: 0.08528531122445597\n",
      "Epoch: 41, Loss: 2.0869888508885728, Val Loss: 0.08513402126848958\n",
      "Epoch: 42, Loss: 2.0809697609534825, Val Loss: 0.08498218771481954\n",
      "Epoch: 43, Loss: 2.074958091275308, Val Loss: 0.08482979467784678\n",
      "Epoch: 44, Loss: 2.0689536026594935, Val Loss: 0.08467681992501443\n",
      "Epoch: 45, Loss: 2.062955342538807, Val Loss: 0.08452329135352772\n",
      "Epoch: 46, Loss: 2.056965329895074, Val Loss: 0.08436920115430294\n",
      "Epoch: 47, Loss: 2.0509827132192657, Val Loss: 0.08421446909309876\n",
      "Epoch: 48, Loss: 2.045006713825589, Val Loss: 0.08405909466696665\n",
      "Epoch: 49, Loss: 2.039037193392229, Val Loss: 0.08390307143845918\n",
      "Epoch: 50, Loss: 2.033072801069492, Val Loss: 0.08374637986076451\n",
      "Epoch: 51, Loss: 2.027115034868775, Val Loss: 0.08358904934430048\n",
      "Epoch: 52, Loss: 2.0211638672280716, Val Loss: 0.08343105689549303\n",
      "Epoch: 53, Loss: 2.015219126043061, Val Loss: 0.08327240597939213\n",
      "Epoch: 54, Loss: 2.009281529039382, Val Loss: 0.08311309041786412\n",
      "Epoch: 55, Loss: 2.003350466292074, Val Loss: 0.08295313291214025\n",
      "Epoch: 56, Loss: 1.997426703289617, Val Loss: 0.08279249998429174\n",
      "Epoch: 57, Loss: 1.9915102431855443, Val Loss: 0.0826312266933633\n",
      "Epoch: 58, Loss: 1.985600611270414, Val Loss: 0.08246927281584934\n",
      "Epoch: 59, Loss: 1.9796981235529123, Val Loss: 0.0823066398988561\n",
      "Epoch: 60, Loss: 1.9738029962075123, Val Loss: 0.08214331158870666\n",
      "Epoch: 61, Loss: 1.9679141108185216, Val Loss: 0.0819792785620258\n",
      "Epoch: 62, Loss: 1.9620327853968975, Val Loss: 0.08181456999634318\n",
      "Epoch: 63, Loss: 1.9561588371534104, Val Loss: 0.08164914012877286\n",
      "Epoch: 64, Loss: 1.9502913532387944, Val Loss: 0.0814829582552149\n",
      "Epoch: 65, Loss: 1.9444294693341355, Val Loss: 0.08131607090697601\n",
      "Epoch: 66, Loss: 1.9385751496479968, Val Loss: 0.08114849068273453\n",
      "Epoch: 67, Loss: 1.9327279431356839, Val Loss: 0.08098022666320238\n",
      "Epoch: 68, Loss: 1.9268884286515096, Val Loss: 0.08081133299581378\n",
      "Epoch: 69, Loss: 1.9210580680474036, Val Loss: 0.08064181494714745\n",
      "Epoch: 70, Loss: 1.9152358307386783, Val Loss: 0.08047167580392871\n",
      "Epoch: 71, Loss: 1.9094229871519186, Val Loss: 0.08030085808262692\n",
      "Epoch: 72, Loss: 1.9036186792627445, Val Loss: 0.08012931906519413\n",
      "Epoch: 73, Loss: 1.8978223113061297, Val Loss: 0.07995714273405494\n",
      "Epoch: 74, Loss: 1.892035064135922, Val Loss: 0.07978433530787848\n",
      "Epoch: 75, Loss: 1.886257778085269, Val Loss: 0.07961091566364431\n",
      "Epoch: 76, Loss: 1.880490366643177, Val Loss: 0.0794368806855079\n",
      "Epoch: 77, Loss: 1.8747327353076668, Val Loss: 0.07926222849564438\n",
      "Epoch: 78, Loss: 1.8689849242583116, Val Loss: 0.07908700278907842\n",
      "Epoch: 79, Loss: 1.863247830316062, Val Loss: 0.0789112036432786\n",
      "Epoch: 80, Loss: 1.8575221319897408, Val Loss: 0.0787348263124684\n",
      "Epoch: 81, Loss: 1.8518083413069124, Val Loss: 0.07855787549510965\n",
      "Epoch: 82, Loss: 1.8461066364364178, Val Loss: 0.07838033189413715\n",
      "Epoch: 83, Loss: 1.840416650733309, Val Loss: 0.0782022421025525\n",
      "Epoch: 84, Loss: 1.8347398414937859, Val Loss: 0.07802364632920833\n",
      "Epoch: 85, Loss: 1.8290760145215117, Val Loss: 0.07784451115232509\n",
      "Epoch: 86, Loss: 1.8234255034252298, Val Loss: 0.07766488191529987\n",
      "Epoch: 87, Loss: 1.8177890004812043, Val Loss: 0.07748476003922214\n",
      "Epoch: 88, Loss: 1.8121676012480512, Val Loss: 0.07730413374194979\n",
      "Epoch: 89, Loss: 1.8065602930854643, Val Loss: 0.07712303147342375\n",
      "Epoch: 90, Loss: 1.800967739605796, Val Loss: 0.07694143271033455\n",
      "Epoch: 91, Loss: 1.7953898942106516, Val Loss: 0.07675940006014781\n",
      "Epoch: 92, Loss: 1.7898282654339184, Val Loss: 0.07657697661474641\n",
      "Epoch: 93, Loss: 1.7842838515542638, Val Loss: 0.07639417105855346\n",
      "Epoch: 94, Loss: 1.7787551400402961, Val Loss: 0.0762109440809757\n",
      "Epoch: 95, Loss: 1.7732426905759286, Val Loss: 0.07602730229232252\n",
      "Epoch: 96, Loss: 1.7677474316767765, Val Loss: 0.07584333646699801\n",
      "Epoch: 97, Loss: 1.7622705156075877, Val Loss: 0.07565904374055371\n",
      "Epoch: 98, Loss: 1.756810821762973, Val Loss: 0.0754744288392362\n",
      "Epoch: 99, Loss: 1.7513692255990019, Val Loss: 0.0752895223710531\n",
      "Epoch: 100, Loss: 1.7459467456860573, Val Loss: 0.07510435086431064\n",
      "Epoch: 101, Loss: 1.7405430738715928, Val Loss: 0.07491890944423038\n",
      "Epoch: 102, Loss: 1.7351589242187972, Val Loss: 0.07473321123264687\n",
      "Epoch: 103, Loss: 1.7297941769734901, Val Loss: 0.07454727819631914\n",
      "Epoch: 104, Loss: 1.7244498927301493, Val Loss: 0.07436119720150733\n",
      "Epoch: 105, Loss: 1.7191263822693619, Val Loss: 0.07417494038533701\n",
      "Epoch: 106, Loss: 1.7138238381481627, Val Loss: 0.07398854018147943\n",
      "Epoch: 107, Loss: 1.7085424131105291, Val Loss: 0.07380206359859022\n",
      "Epoch: 108, Loss: 1.703283378360534, Val Loss: 0.07361551396566965\n",
      "Epoch: 109, Loss: 1.6980474038171982, Val Loss: 0.0734289035861558\n",
      "Epoch: 110, Loss: 1.6928344889790807, Val Loss: 0.07324230175548363\n",
      "Epoch: 111, Loss: 1.6876460096980022, Val Loss: 0.0730557387144943\n",
      "Epoch: 112, Loss: 1.6824822357237166, Val Loss: 0.0728692294712437\n",
      "Epoch: 113, Loss: 1.677344389091082, Val Loss: 0.07268281685987198\n",
      "Epoch: 114, Loss: 1.672231852623151, Val Loss: 0.07249655047888433\n",
      "Epoch: 115, Loss: 1.6671446463154806, Val Loss: 0.07231047373980677\n",
      "Epoch: 116, Loss: 1.6620839807120382, Val Loss: 0.07212464809768666\n",
      "Epoch: 117, Loss: 1.6570503183354663, Val Loss: 0.07193910188628036\n",
      "Epoch: 118, Loss: 1.6520436652990618, Val Loss: 0.07175388743676031\n",
      "Epoch: 119, Loss: 1.6470638075220825, Val Loss: 0.07156903024577974\n",
      "Epoch: 120, Loss: 1.6421099564295145, Val Loss: 0.071384557741602\n",
      "Epoch: 121, Loss: 1.6371829595293776, Val Loss: 0.07120052264223614\n",
      "Epoch: 122, Loss: 1.6322837063145652, Val Loss: 0.07101696780211271\n",
      "Epoch: 123, Loss: 1.6274125367407368, Val Loss: 0.0708339538147047\n",
      "Epoch: 124, Loss: 1.622569764050147, Val Loss: 0.07065145175083573\n",
      "Epoch: 125, Loss: 1.6177550773408758, Val Loss: 0.07046957495312896\n",
      "Epoch: 126, Loss: 1.6129693963130072, Val Loss: 0.07028831151577142\n",
      "Epoch: 127, Loss: 1.6082122397812686, Val Loss: 0.07010773811302846\n",
      "Epoch: 128, Loss: 1.603484401278451, Val Loss: 0.06992787493366974\n",
      "Epoch: 129, Loss: 1.5987864295727903, Val Loss: 0.06974881553167987\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5156666666666667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, numpy as np, matplotlib.pyplot as plt\n",
    "folder = \"/kaggle/input/fashion-mnist/final\" # folder containing the data\n",
    "\n",
    "def load_data(X, y):\n",
    "    for f in os.listdir(folder):\n",
    "        for file in os.listdir(f\"{folder}/{f}\"):\n",
    "            img = plt.imread(f\"{folder}/{f}/{file}\")\n",
    "            X.append(img)\n",
    "\n",
    "            # The most obvious choice for the label is the class (folder) name\n",
    "            # label = int(f)\n",
    "\n",
    "            # [Q1] But we dont use it here why? Why is it an array of 10 elements?\n",
    "            # Clue: Lookup one hot encoding\n",
    "            # Read up on Cross Entropy Loss\n",
    "\n",
    "            label = [0] * 10\n",
    "            label[int(f)] = 1 # Why is this array the label and not a numeber?\n",
    "\n",
    "            y.append(label)\n",
    "            \n",
    "        print(f\"Loaded {f} class\")\n",
    "\n",
    "X, y = [], []\n",
    "load_data(X, y)\n",
    "\n",
    "# [Q2] Why convert to numpy array?\n",
    "X=np.array(X)\n",
    "y=np.array(y)\n",
    "\n",
    "X = X[:, :,:, 0] # [Q3] Why are we doing this and what does this type of slicing result in?\n",
    "X = X.reshape(X.shape[0], X.shape[1]*X.shape[2]) # [Q4] Why are we reshaping the data?\n",
    "print(\"After reshaping\")\n",
    "print(X.shape, y.shape)\n",
    "print(X[0], y[0])\n",
    "\n",
    "class NN:\n",
    "    def __init__(self, input_neurons, hidden_neurons, output_neurons, learning_rate, epochs):\n",
    "        \"\"\"\n",
    "        Class Definition\n",
    "        \n",
    "        We use a class because it is easy to visualize the process of training a neural network\n",
    "        It's also easier to resuse and repurpose depending on the task at hand\n",
    "\n",
    "        We have a simple neural network, with an input layer, one hidden (middle) layer and an output layer\n",
    "\n",
    "        input_neurons: Number of neurons in the input layer\n",
    "        hidden_neurons: Number of neurons in the hidden layer\n",
    "        output_neurons: Number of neurons in the output layer\n",
    "        learning_rate: The rate at which the weights are updated [Q5] What is the learning rate?\n",
    "        epochs: Number of times the model will train on the entire dataset \n",
    "        \"\"\"\n",
    "\n",
    "        self.input_neurons = input_neurons\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.output_neurons = output_neurons\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        \"\"\"\n",
    "        Weights and Biases\n",
    "\n",
    "        At this point you should know what weights and biases are in a neural network and if not, go check out the 3blue1brown video on Neural Networks\n",
    "        What matters here is however the matrix dimensions of the weights and biases\n",
    "\n",
    "         [Q6] Why are the dimensions of the weights and biases the way they are? \n",
    "        \n",
    "        Try to figure out the dimensions of the weights and biases for the hidden and output layers\n",
    "        Try to see what equations represent the forward pass (basically the prediction)\n",
    "        And then, try to see if the dimensions of the matrix multiplications are correct\n",
    "\n",
    "        Note: The bias dimensions may not match. Look up broadcasting in numpy to understand\n",
    "        [Q7] What is broadcasting and why do we need to broadcast the bias?\n",
    "        \"\"\"\n",
    "\n",
    "        # Ideally any random set of weights and biases can be used to initialize the network\n",
    "        # self.wih = np.random.randn(hidden_neurons, input_neurons)\n",
    "\n",
    "        # [Q8] What is np.random.randn? What's the shape of this matrix?\n",
    "\n",
    "        # Optional: Try to figure out why the weights are initialized this way\n",
    "        # Note: You can just use the commented out line above if you don't want to do this\n",
    "\n",
    "        self.wih = np.random.randn(hidden_neurons, input_neurons) * np.sqrt(2/input_neurons)\n",
    "        self.bih = np.zeros((hidden_neurons, 1))\n",
    "\n",
    "        self.who = np.random.randn(output_neurons, hidden_neurons) * np.sqrt(2/hidden_neurons)\n",
    "        self.bho = np.zeros((output_neurons, 1))\n",
    "\n",
    "    # Activation Functions and their derivatives\n",
    "    # [Q9] What are activation functions and why do we need them?\n",
    "\n",
    "    def relu(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the RELU function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return z * (z > 0)\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the Sigmoid function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def relu_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the RELU derivative function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return 1 * (z > 0)\n",
    "\n",
    "    def sigmoid_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the Sigmoid derivative function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return z * (1 - z)\n",
    "\n",
    "    # [Q10] What is the softmax function and why do we need it? Read up on it\n",
    "    def softmax(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the Softmax function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "\n",
    "    def softmax_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the Softmax derivative function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return z * (1 - z)\n",
    "\n",
    "    # Loss Functions and their derivatives\n",
    "    # [Q11] What are loss functions and why do we need them?\n",
    "\n",
    "    def mean_squared_error(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implementation of the Mean Squared Error function\n",
    "        y: (10, n)\n",
    "        y_hat: (10, n)\n",
    "        returns (1, n)\n",
    "        \"\"\"\n",
    "        return np.mean((y - y_hat) ** 2, axis=0)\n",
    "\n",
    "    def cross_entropy_loss(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implementation of the Cross Entropy Loss function\n",
    "        y: (10, n)\n",
    "        y_hat: (10, n)\n",
    "        returns (1, n)\n",
    "        \"\"\"\n",
    "\n",
    "        # Implement the cross entropy loss function here and return it\n",
    "        # Keep the dimensions of the input in mind when writing the code\n",
    "\n",
    "        # [Code Goes Here]\n",
    "        logits, batch_size = y.shape\n",
    "        loss = []\n",
    "        epsilon = 1e-9  # I got runtime error which I debugged and found out this log(0) problem so had to declare this using help of internet\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            t = 0\n",
    "            for j in range(logits):\n",
    "                t += -y[j, i] * np.log(y_hat[j, i] + epsilon)\n",
    "            loss.append(t)\n",
    "        loss = np.array([loss])\n",
    "        return loss\n",
    "        \n",
    "    def mean_squared_error_derivative(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implementation of the Mean Squared Error derivative function\n",
    "        y: (10, n)\n",
    "        y_hat: (10, n)\n",
    "        returns (10, n)\n",
    "        \"\"\"\n",
    "        return y_hat - y\n",
    "\n",
    "    def cross_entropy_derivative(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implementation of the Cross Entropy Loss derivative function\n",
    "        y: (10, n)\n",
    "        y_hat: (10, n)\n",
    "        returns (10, n)\n",
    "        \"\"\"\n",
    "\n",
    "        # Implement the cross entropy loss derivative function here and return it\n",
    "        # Note: The derivative of the CEL is usually taken with respect to the softmax input not output so keep that in mind while writing\n",
    "\n",
    "        # [Code Goes Here]\n",
    "        cross_entropy_der = y_hat - y\n",
    "\n",
    "        return cross_entropy_der\n",
    "    # Forward propagation\n",
    "    def forward(self, input_list):\n",
    "        \"\"\"\n",
    "        Implementation of the Forward Pass\n",
    "        input_list: (784, n)        - n is the number of images\n",
    "        returns (10, n)              - n is the number of images\n",
    "    \n",
    "        Now we come to the heart of the neural network, the forward pass\n",
    "        This is where the input is passed through the network to get the output\n",
    "\n",
    "        [Q12] What does the output choice we have here mean? It's an array of 10 elements per image, but why?\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = np.array(input_list, ndmin=2).T\n",
    "        inputs = inputs - np.mean(inputs) # [Q13] Why are we subtracting the mean of the inputs?\n",
    "        \n",
    "        # To get to the hidden layer:\n",
    "        # Multiply the input with the weights and adding the bias\n",
    "        # Apply the activation function (relu in this case)\n",
    "\n",
    "        # [Code Goes Here]\n",
    "        z_hidden = np.dot(self.wih, inputs) + self.bih\n",
    "        a_hidden = self.relu(z_hidden)\n",
    "\n",
    "\n",
    "        # To get to the output layer:\n",
    "        # Multiply the hidden layer output with the weights and adding the bias\n",
    "        # Apply the activation function (softmax in this case)\n",
    "        # [Q14] Why are we using the softmax function here?\n",
    "\n",
    "        # [Code Goes Here]\n",
    "        z_output = np.dot(self.who, a_hidden) + self.bho\n",
    "        y_hat = self.softmax(z_output)\n",
    "\n",
    "\n",
    "        # Return it\n",
    "\n",
    "        # [Code Goes Here]\n",
    "        return y_hat\n",
    "        \n",
    "\n",
    "    # Back propagation\n",
    "    def backprop(self, inputs_list, targets_list):\n",
    "        \"\"\"\n",
    "        Implementation of the Backward Pass\n",
    "        inputs_list: (784, n)\n",
    "        targets_list: (10, n)\n",
    "        returns a scalar value (loss)\n",
    "        \n",
    "        This is where the magic happens, the backpropagation algorithm\n",
    "        This is where the weights are updated based on the error in the prediction of the network\n",
    "\n",
    "        Now, the calculus involved is fairly complicated, especially because it's being done in matrix form\n",
    "        However the intuition is simple. \n",
    "\n",
    "        Since this is a recruitment stage, most of the function is written out for you, so follow along with the comments\n",
    "        \"\"\"\n",
    "\n",
    "        # Basic forward pass to get the outputs\n",
    "        # Obviously we need the predictions to know how the model is doing\n",
    "        # [Q15] Why are we doing a forward pass here instead of just using the outputs from the forward function?\n",
    "        # Is there any actual reason, or could we just swap it?\n",
    "\n",
    "        inputs = np.array(inputs_list, ndmin=2).T # (784, n)\n",
    "        inputs = inputs - np.mean(inputs)\n",
    "\n",
    "        tj = np.array(targets_list, ndmin=2).T # (10, n)\n",
    "\n",
    "        hidden_inputs = np.dot(self.wih, inputs) + self.bih\n",
    "        hidden_outputs = self.relu(hidden_inputs)\n",
    "\n",
    "        final_inputs = np.dot(self.who, hidden_outputs) + self.bho\n",
    "        yj = self.softmax(final_inputs)\n",
    "\n",
    "        # Calculating the loss - This is the error in the prediction\n",
    "        # The loss then is the indication of how well the model is doing, its a useful parameter to track to see if the model is improving\n",
    "\n",
    "        loss = self.cross_entropy_loss(tj, yj) # Convert this to cross entropy loss\n",
    "\n",
    "\n",
    "        # Updating the weights using Update Rule\n",
    "        # Now that we have the incorrect predictions, we can update the weights to make the predictions better\n",
    "        # This is done using the gradient of the loss function with respect to the weights\n",
    "        # Basically, we know how much the overall error is caused due to individual weights using the chain rule of calculus\n",
    "        # Since we want to minimise the error, we move in the opposite direction of something like a \"derivative\" of the error with respect to the weights\n",
    "        # Calculus therefore helps us find the direction in which we should move to reduce the error\n",
    "        # A direction means what delta W changes we need to make to make the model better\n",
    "\n",
    "        # Output Layer - We start with the output layer because we are backtracking how the error is caused\n",
    "        # Think of it as using the derivatives of each layer while going back\n",
    "\n",
    "\n",
    "        # For the task, you will be using Cross Entropy Loss\n",
    "\n",
    "        # Change this to cross entropy loss\n",
    "        dE_dzo = self.mean_squared_error_derivative(tj, yj) * self.softmax_derivative(yj) # (10,n)\n",
    "        # Note: the derivative of the CEL is usually taken with respect to the softmax input not output so keep that in mind while writing\n",
    "\n",
    "\n",
    "        dE_dwho = np.dot(dE_dzo, hidden_outputs.T) / hidden_outputs.shape[1] # dot((10,n) (n,128) = (10,128)\n",
    "        dE_dbho = np.mean(dE_dzo, axis=1, keepdims=True) # sum((10,n), axis=1) = (10,1)\n",
    "        \n",
    "        self.who -= self.lr * dE_dwho\n",
    "        self.bho -= self.lr * dE_dbho\n",
    "\n",
    "        # Hidden Layer\n",
    "        dE_dah = np.dot(self.who.T, dE_dzo) # dot((128,10), (10,n)) = (128,n)\n",
    "        dE_dzh = dE_dah * self.relu_derivative(hidden_inputs)\n",
    "        dE_dwih = np.dot(dE_dzh, inputs.T) / inputs.shape[1]\n",
    "        dE_dbih = np.mean(dE_dzh, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "        self.wih -= self.lr * dE_dwih\n",
    "        self.bih -= self.lr * dE_dbih\n",
    "\n",
    "        return np.mean(loss)\n",
    "\n",
    "    def fit(self, inputs_list, targets_list,validation_data, validation_labels):\n",
    "        \"\"\"\n",
    "        Implementation of the training loop\n",
    "        inputs_list: (784, n)\n",
    "        targets_list: (10, n)\n",
    "        validation_data: (784, n)\n",
    "        validation_labels: (10, n)\n",
    "        returns train_loss, val_loss\n",
    "\n",
    "        This is where the training loop is implemented\n",
    "        We loop over the entire dataset for a certain number of epochs\n",
    "        We also track the validation loss to see how well the model is generalizing\n",
    "        [Q16] What is the validation dataset and what do we mean by generalization?\n",
    "\n",
    "        We also return the training and validation loss to see how the model is improving\n",
    "        It's a good idea to plot these to see how the model is doing\n",
    "        \"\"\"\n",
    "\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "        for epoch in range(self.epochs):\n",
    "            loss = self.backprop(inputs_list, targets_list)\n",
    "            train_loss.append(loss)\n",
    "            vloss = self.mean_squared_error(validation_labels.T, self.forward(validation_data))\n",
    "            val_loss.append(np.mean(vloss)) \n",
    "            print(f\"Epoch: {epoch}, Loss: {loss}, Val Loss: {val_loss[-1]}\")\n",
    "\n",
    "        return train_loss[1:], val_loss[:-1] \n",
    "\n",
    "    def predict(self, X):\n",
    "        outputs = self.forward(X).T\n",
    "        return outputs\n",
    "\n",
    "# This is where the class is used to train the model\n",
    "\n",
    "# The parameters in the model are (input_neurons, hidden_neurons, output_neurons, learning_rate, epochs)\n",
    "# These parameters aren't the right parameters, so tweak them to get the best results\n",
    "# Around 70% accuracy is a good end goal (75% is great) but for the recruitment task, 60% is good enough\n",
    "\n",
    "# [Q17] What are the parameters in the model and what do they mean?\n",
    "\n",
    "fashion_mnist = NN(784, 160, 10, 0.03, 130)\n",
    "p = np.random.permutation(len(X))\n",
    "X, y = X[p], y[p]\n",
    "\n",
    "# Splitting the data into training, validation and testing in the ratio 70:20:10\n",
    "X_train, y_train = X[:int(0.7*len(X))], y[:int(0.7*len(X))]\n",
    "X_val, y_val = X[int(0.7*len(X)):int(0.9*len(X))], y[int(0.7*len(X)):int(0.9*len(X))]\n",
    "X_test, y_test = X[int(0.9*len(X)):], y[int(0.9*len(X)):]\n",
    "\n",
    "# Training the model\n",
    "train_loss,val_loss = fashion_mnist.fit(X_train, y_train,X_val,y_val)\n",
    "\n",
    "\n",
    "# Plotting the loss\n",
    "plt.plot(train_loss,label='train')\n",
    "plt.plot(val_loss,label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred = fashion_mnist.predict(X_test)\n",
    "\n",
    "# [Q18] Why are we using argmax here? Why is this output different from the output of the model?\n",
    "y_pred = np.argmax(y_pred, axis=1)  \n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "print(f\"Accuracy: {np.mean(y_pred == y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8317357,
     "sourceId": 13129288,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 466.825814,
   "end_time": "2025-09-21T19:50:57.990361",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-21T19:43:11.164547",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
