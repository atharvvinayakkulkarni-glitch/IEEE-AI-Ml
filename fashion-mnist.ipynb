{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c614ca8d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-21T19:41:51.695005Z",
     "iopub.status.busy": "2025-09-21T19:41:51.694545Z",
     "iopub.status.idle": "2025-09-21T19:49:16.100466Z",
     "shell.execute_reply": "2025-09-21T19:49:16.099205Z"
    },
    "papermill": {
     "duration": 444.411332,
     "end_time": "2025-09-21T19:49:16.102056",
     "exception": false,
     "start_time": "2025-09-21T19:41:51.690724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7 class\n",
      "Loaded 2 class\n",
      "Loaded 5 class\n",
      "Loaded 8 class\n",
      "Loaded 0 class\n",
      "Loaded 3 class\n",
      "Loaded 1 class\n",
      "Loaded 4 class\n",
      "Loaded 9 class\n",
      "Loaded 6 class\n",
      "After reshaping\n",
      "(60000, 784) (60000, 10)\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00392157 0.         0.         0.1254902  0.1882353\n",
      " 0.         0.52156866 0.28627452 0.         0.         0.\n",
      " 0.         0.         0.         0.01568628 0.1254902  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.01960784 0.\n",
      " 0.00392157 0.15686275 0.5254902  0.7019608  0.5294118  0.4627451\n",
      " 0.69803923 0.25490198 0.         0.01960784 0.         0.05882353\n",
      " 0.1882353  0.2509804  0.5254902  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.00392157 0.01568628 0.         0.         0.35686275 0.5137255\n",
      " 0.10588235 0.38039216 0.78431374 0.4117647  0.54509807 0.5254902\n",
      " 0.         0.         0.         0.3647059  0.7372549  0.7607843\n",
      " 0.2784314  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.01568628 0.         0.\n",
      " 0.07843138 0.3137255  0.5568628  0.4745098  0.09803922 0.5137255\n",
      " 0.627451   0.6745098  0.63529414 0.54901963 0.4862745  0.21176471\n",
      " 0.2509804  0.654902   0.6509804  0.69803923 0.09803922 0.\n",
      " 0.         0.         0.00392157 0.03137255 0.01960784 0.01960784\n",
      " 0.         0.         0.02745098 0.3372549  0.40784314 0.64705884\n",
      " 0.17254902 0.5921569  0.88235295 0.93333334 0.7176471  0.53333336\n",
      " 0.5686275  0.7019608  0.8039216  0.81960785 0.7647059  0.65882355\n",
      " 0.70980394 0.6901961  0.06666667 0.         0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.14901961\n",
      " 0.36078432 0.5764706  0.28235295 0.4745098  0.6156863  0.6156863\n",
      " 0.8509804  0.8235294  0.67058825 0.8039216  0.7254902  0.6901961\n",
      " 0.59607846 0.7176471  0.8        0.7607843  0.6745098  0.7607843\n",
      " 0.09803922 0.         0.         0.         0.         0.15294118\n",
      " 0.3372549  0.4745098  0.67058825 0.70980394 0.42745098 0.38039216\n",
      " 0.61960787 0.6509804  0.50980395 0.64705884 0.6313726  0.45490196\n",
      " 0.94509804 0.654902   0.5372549  0.8901961  0.61960787 0.5686275\n",
      " 0.8117647  0.78431374 0.6862745  0.8235294  0.26666668 0.\n",
      " 0.10588235 0.6        0.2        0.5254902  0.64705884 0.58431375\n",
      " 0.5058824  0.37254903 0.47058824 0.58431375 0.56078434 0.7529412\n",
      " 0.7647059  0.27058825 0.68235296 0.4392157  0.6392157  0.8901961\n",
      " 0.75686276 0.87058824 0.8509804  0.7882353  0.7921569  0.80784315\n",
      " 0.79607844 0.7921569  0.69411767 0.         0.34901962 0.64705884\n",
      " 0.5254902  0.25490198 0.14901961 0.3254902  0.40784314 0.57254905\n",
      " 0.5372549  0.6666667  0.4745098  0.23137255 0.8        0.7137255\n",
      " 0.44313726 1.         0.7921569  0.85882354 0.8156863  0.64705884\n",
      " 0.7019608  0.7294118  0.70980394 0.74509805 0.78431374 0.73333335\n",
      " 0.80784315 0.08235294 0.36862746 0.7019608  0.6392157  0.7294118\n",
      " 0.74509805 0.69803923 0.59607846 0.56078434 0.5568628  0.5764706\n",
      " 0.7294118  0.70980394 0.7647059  0.88235295 0.8784314  0.70980394\n",
      " 0.69411767 0.5921569  0.6392157  0.65882355 0.6313726  0.6392157\n",
      " 0.5921569  0.62352943 0.627451   0.70980394 0.69411767 0.\n",
      " 0.         0.6313726  0.69411767 0.5254902  0.63529414 0.6666667\n",
      " 0.7372549  0.7137255  0.6901961  0.6156863  0.654902   0.73333335\n",
      " 0.74509805 0.68235296 0.6313726  0.64705884 0.67058825 0.73333335\n",
      " 0.8039216  0.3764706  0.28627452 0.7490196  0.7254902  0.78431374\n",
      " 0.5137255  0.5137255  0.85882354 0.17254902 0.         0.\n",
      " 0.31764707 0.35686275 0.47058824 0.9647059  0.5882353  0.34509805\n",
      " 0.67058825 0.8509804  0.5058824  0.4        0.46666667 0.73333335\n",
      " 0.91764706 0.75686276 0.7607843  0.8117647  0.69411767 0.3764706\n",
      " 0.34117648 0.6392157  0.87058824 0.7294118  0.28627452 0.34509805\n",
      " 0.74509805 0.5058824  0.00392157 0.         0.         0.06666667\n",
      " 0.09019608 0.3254902  0.3019608  0.28627452 0.45490196 0.5568628\n",
      " 0.28627452 0.3137255  0.24313726 0.24705882 0.48235294 0.\n",
      " 0.         0.         0.15686275 0.28235295 0.21176471 0.43529412\n",
      " 0.4        0.19607843 0.30588236 0.28235295 0.2        0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ] [0 0 0 0 0 0 0 1 0 0]\n",
      "Epoch: 0, Loss: 2.4423123872575205, Val Loss: 0.0927186496455391\n",
      "Epoch: 1, Loss: 2.431636296811216, Val Loss: 0.09247661899689345\n",
      "Epoch: 2, Loss: 2.4212860016493463, Val Loss: 0.09224276126924065\n",
      "Epoch: 3, Loss: 2.4112363380298514, Val Loss: 0.09201627091688246\n",
      "Epoch: 4, Loss: 2.4014579036910275, Val Loss: 0.0917963675291703\n",
      "Epoch: 5, Loss: 2.391926509070669, Val Loss: 0.09158246728500989\n",
      "Epoch: 6, Loss: 2.3826241592195645, Val Loss: 0.09137397655074764\n",
      "Epoch: 7, Loss: 2.373533478758002, Val Loss: 0.09117048410445006\n",
      "Epoch: 8, Loss: 2.3646410602197636, Val Loss: 0.09097149106792662\n",
      "Epoch: 9, Loss: 2.3559318335173898, Val Loss: 0.09077655794592308\n",
      "Epoch: 10, Loss: 2.34738961513492, Val Loss: 0.09058532326499373\n",
      "Epoch: 11, Loss: 2.3390043226097283, Val Loss: 0.09039743387118995\n",
      "Epoch: 12, Loss: 2.330764019441362, Val Loss: 0.09021259855718476\n",
      "Epoch: 13, Loss: 2.3226601464273893, Val Loss: 0.09003054221639796\n",
      "Epoch: 14, Loss: 2.314682223387944, Val Loss: 0.08985095902556323\n",
      "Epoch: 15, Loss: 2.306822361174746, Val Loss: 0.08967361261391298\n",
      "Epoch: 16, Loss: 2.299073595943988, Val Loss: 0.08949828125779345\n",
      "Epoch: 17, Loss: 2.2914301120893548, Val Loss: 0.08932477214162918\n",
      "Epoch: 18, Loss: 2.283885286736665, Val Loss: 0.08915293934116585\n",
      "Epoch: 19, Loss: 2.27643302670181, Val Loss: 0.08898262536170543\n",
      "Epoch: 20, Loss: 2.269068138240396, Val Loss: 0.08881364835043783\n",
      "Epoch: 21, Loss: 2.2617857070103398, Val Loss: 0.0886457876788712\n",
      "Epoch: 22, Loss: 2.2545786147471136, Val Loss: 0.08847894036301159\n",
      "Epoch: 23, Loss: 2.2474423449997247, Val Loss: 0.08831299131379947\n",
      "Epoch: 24, Loss: 2.2403738706069776, Val Loss: 0.08814783794284134\n",
      "Epoch: 25, Loss: 2.233370211870951, Val Loss: 0.08798340020362874\n",
      "Epoch: 26, Loss: 2.226428554687488, Val Loss: 0.08781955067544321\n",
      "Epoch: 27, Loss: 2.2195445053872893, Val Loss: 0.08765619291953441\n",
      "Epoch: 28, Loss: 2.2127158367468507, Val Loss: 0.0874932313007267\n",
      "Epoch: 29, Loss: 2.2059392528654023, Val Loss: 0.0873306091251883\n",
      "Epoch: 30, Loss: 2.1992124443371908, Val Loss: 0.08716827952242537\n",
      "Epoch: 31, Loss: 2.1925340519826535, Val Loss: 0.08700619382518295\n",
      "Epoch: 32, Loss: 2.1859025461100035, Val Loss: 0.08684426617933029\n",
      "Epoch: 33, Loss: 2.179314935885435, Val Loss: 0.0866824338754592\n",
      "Epoch: 34, Loss: 2.172770374777419, Val Loss: 0.08652065964094091\n",
      "Epoch: 35, Loss: 2.166267884642748, Val Loss: 0.08635887996939189\n",
      "Epoch: 36, Loss: 2.1598049302967444, Val Loss: 0.08619713644117576\n",
      "Epoch: 37, Loss: 2.153381980404394, Val Loss: 0.0860353527854799\n",
      "Epoch: 38, Loss: 2.146995524742969, Val Loss: 0.08587349292441002\n",
      "Epoch: 39, Loss: 2.1406458791851444, Val Loss: 0.08571157030147576\n",
      "Epoch: 40, Loss: 2.1343331897077817, Val Loss: 0.0855495260510104\n",
      "Epoch: 41, Loss: 2.1280545328937, Val Loss: 0.08538740287923115\n",
      "Epoch: 42, Loss: 2.121810352972782, Val Loss: 0.08522516300704809\n",
      "Epoch: 43, Loss: 2.1155979777994522, Val Loss: 0.08506280334304962\n",
      "Epoch: 44, Loss: 2.109418440183685, Val Loss: 0.08490029035304669\n",
      "Epoch: 45, Loss: 2.103269074974451, Val Loss: 0.08473764043224918\n",
      "Epoch: 46, Loss: 2.0971500722428993, Val Loss: 0.08457488379392934\n",
      "Epoch: 47, Loss: 2.0910615329072297, Val Loss: 0.08441197848756503\n",
      "Epoch: 48, Loss: 2.08500234849591, Val Loss: 0.08424891253905431\n",
      "Epoch: 49, Loss: 2.0789720227412882, Val Loss: 0.08408570509782948\n",
      "Epoch: 50, Loss: 2.0729706186628394, Val Loss: 0.08392235959511309\n",
      "Epoch: 51, Loss: 2.066998264219345, Val Loss: 0.083758937279475\n",
      "Epoch: 52, Loss: 2.06105474125177, Val Loss: 0.08359542761501709\n",
      "Epoch: 53, Loss: 2.055138321897054, Val Loss: 0.08343184007539033\n",
      "Epoch: 54, Loss: 2.0492508692526568, Val Loss: 0.08326822572432148\n",
      "Epoch: 55, Loss: 2.043390524064788, Val Loss: 0.08310451130677173\n",
      "Epoch: 56, Loss: 2.0375556189164232, Val Loss: 0.08294076335158526\n",
      "Epoch: 57, Loss: 2.0317479240763765, Val Loss: 0.08277699952943583\n",
      "Epoch: 58, Loss: 2.0259670512160644, Val Loss: 0.08261325504702345\n",
      "Epoch: 59, Loss: 2.0202120947933375, Val Loss: 0.0824496204819184\n",
      "Epoch: 60, Loss: 2.014483392070417, Val Loss: 0.08228607530939473\n",
      "Epoch: 61, Loss: 2.0087801738813917, Val Loss: 0.08212261556369774\n",
      "Epoch: 62, Loss: 2.0031013848296673, Val Loss: 0.0819593098896908\n",
      "Epoch: 63, Loss: 1.9974476642587813, Val Loss: 0.08179615422800915\n",
      "Epoch: 64, Loss: 1.991819295716801, Val Loss: 0.08163316595588874\n",
      "Epoch: 65, Loss: 1.9862169552784426, Val Loss: 0.08147034325222882\n",
      "Epoch: 66, Loss: 1.9806393747861228, Val Loss: 0.08130769763554313\n",
      "Epoch: 67, Loss: 1.975084943035553, Val Loss: 0.08114519687090767\n",
      "Epoch: 68, Loss: 1.969553682851752, Val Loss: 0.08098290287765286\n",
      "Epoch: 69, Loss: 1.9640465985412434, Val Loss: 0.08082082077905847\n",
      "Epoch: 70, Loss: 1.9585619459756767, Val Loss: 0.08065897307794512\n",
      "Epoch: 71, Loss: 1.9530992892722134, Val Loss: 0.08049736829629969\n",
      "Epoch: 72, Loss: 1.94765802113227, Val Loss: 0.08033602332106328\n",
      "Epoch: 73, Loss: 1.9422384496176477, Val Loss: 0.08017495834079766\n",
      "Epoch: 74, Loss: 1.9368412580714396, Val Loss: 0.08001415424208674\n",
      "Epoch: 75, Loss: 1.9314649555990715, Val Loss: 0.0798536460712512\n",
      "Epoch: 76, Loss: 1.9261089286424602, Val Loss: 0.07969344318036575\n",
      "Epoch: 77, Loss: 1.92077273205346, Val Loss: 0.07953354228504238\n",
      "Epoch: 78, Loss: 1.9154563459837033, Val Loss: 0.07937396051047559\n",
      "Epoch: 79, Loss: 1.9101604001308012, Val Loss: 0.07921472356822959\n",
      "Epoch: 80, Loss: 1.9048844068822606, Val Loss: 0.0790558093601972\n",
      "Epoch: 81, Loss: 1.8996267204819777, Val Loss: 0.07889720812706921\n",
      "Epoch: 82, Loss: 1.8943883349404076, Val Loss: 0.07873895432904274\n",
      "Epoch: 83, Loss: 1.8891689887073115, Val Loss: 0.07858101625090105\n",
      "Epoch: 84, Loss: 1.8839675812662044, Val Loss: 0.07842340005330153\n",
      "Epoch: 85, Loss: 1.8787830499235254, Val Loss: 0.07826606971304638\n",
      "Epoch: 86, Loss: 1.8736155513569672, Val Loss: 0.07810903862268496\n",
      "Epoch: 87, Loss: 1.8684649613234041, Val Loss: 0.07795226884143072\n",
      "Epoch: 88, Loss: 1.8633291764700428, Val Loss: 0.07779576076905212\n",
      "Epoch: 89, Loss: 1.8582091472173752, Val Loss: 0.07763950014521653\n",
      "Epoch: 90, Loss: 1.853103702243249, Val Loss: 0.07748347469440546\n",
      "Epoch: 91, Loss: 1.8480128387208643, Val Loss: 0.07732768198186847\n",
      "Epoch: 92, Loss: 1.8429360302414426, Val Loss: 0.07717214165795672\n",
      "Epoch: 93, Loss: 1.8378735451323611, Val Loss: 0.0770168047370037\n",
      "Epoch: 94, Loss: 1.8328254389300516, Val Loss: 0.0768616465885012\n",
      "Epoch: 95, Loss: 1.8277921725306687, Val Loss: 0.07670665471943958\n",
      "Epoch: 96, Loss: 1.8227721537357193, Val Loss: 0.07655178716205228\n",
      "Epoch: 97, Loss: 1.8177642328655215, Val Loss: 0.07639707654053185\n",
      "Epoch: 98, Loss: 1.812769594865093, Val Loss: 0.07624252293048556\n",
      "Epoch: 99, Loss: 1.8077881170598564, Val Loss: 0.07608810784062132\n",
      "Epoch: 100, Loss: 1.8028193173893226, Val Loss: 0.07593384562204443\n",
      "Epoch: 101, Loss: 1.7978636003384398, Val Loss: 0.07577967381584119\n",
      "Epoch: 102, Loss: 1.7929187992559272, Val Loss: 0.07562557624769099\n",
      "Epoch: 103, Loss: 1.7879848966660745, Val Loss: 0.0754715082696806\n",
      "Epoch: 104, Loss: 1.7830614431618486, Val Loss: 0.07531748876332377\n",
      "Epoch: 105, Loss: 1.7781493207350987, Val Loss: 0.07516352958165103\n",
      "Epoch: 106, Loss: 1.773249077597028, Val Loss: 0.07500956942234815\n",
      "Epoch: 107, Loss: 1.7683600053825677, Val Loss: 0.07485563065636044\n",
      "Epoch: 108, Loss: 1.7634828239625993, Val Loss: 0.07470170656453791\n",
      "Epoch: 109, Loss: 1.7586166674219317, Val Loss: 0.07454774059524918\n",
      "Epoch: 110, Loss: 1.7537613916547046, Val Loss: 0.07439374211168345\n",
      "Epoch: 111, Loss: 1.7489171831367445, Val Loss: 0.07423971232214373\n",
      "Epoch: 112, Loss: 1.7440845203669728, Val Loss: 0.07408564756395879\n",
      "Epoch: 113, Loss: 1.7392616896071358, Val Loss: 0.07393151975251167\n",
      "Epoch: 114, Loss: 1.734448866820441, Val Loss: 0.07377737267361446\n",
      "Epoch: 115, Loss: 1.7296471828036475, Val Loss: 0.07362314871488122\n",
      "Epoch: 116, Loss: 1.7248563104918246, Val Loss: 0.07346886109257868\n",
      "Epoch: 117, Loss: 1.7200760726012772, Val Loss: 0.07331448166854086\n",
      "Epoch: 118, Loss: 1.7153065418740583, Val Loss: 0.07315999260572852\n",
      "Epoch: 119, Loss: 1.710547798878085, Val Loss: 0.07300539520276894\n",
      "Epoch: 120, Loss: 1.7058001658828232, Val Loss: 0.07285069688898248\n",
      "Epoch: 121, Loss: 1.7010636213709507, Val Loss: 0.072695884863206\n",
      "Epoch: 122, Loss: 1.696338112675153, Val Loss: 0.07254095321394216\n",
      "Epoch: 123, Loss: 1.6916227686540781, Val Loss: 0.07238589049406918\n",
      "Epoch: 124, Loss: 1.6869180750500097, Val Loss: 0.07223069744284218\n",
      "Epoch: 125, Loss: 1.6822240779509854, Val Loss: 0.07207541462251564\n",
      "Epoch: 126, Loss: 1.6775413794157337, Val Loss: 0.07191997499877764\n",
      "Epoch: 127, Loss: 1.6728694435838847, Val Loss: 0.07176438333292591\n",
      "Epoch: 128, Loss: 1.6682080762847764, Val Loss: 0.0716086092864729\n",
      "Epoch: 129, Loss: 1.663557447517843, Val Loss: 0.07145267712335664\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4691666666666667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, numpy as np, matplotlib.pyplot as plt\n",
    "folder = \"/kaggle/input/fashion-mnist/final\" # folder containing the data\n",
    "\n",
    "def load_data(X, y):\n",
    "    for f in os.listdir(folder):\n",
    "        for file in os.listdir(f\"{folder}/{f}\"):\n",
    "            img = plt.imread(f\"{folder}/{f}/{file}\")\n",
    "            X.append(img)\n",
    "\n",
    "            # The most obvious choice for the label is the class (folder) name\n",
    "            # label = int(f)\n",
    "\n",
    "            # [Q1] But we dont use it here why? Why is it an array of 10 elements?\n",
    "            # Clue: Lookup one hot encoding\n",
    "            # Read up on Cross Entropy Loss\n",
    "\n",
    "            label = [0] * 10\n",
    "            label[int(f)] = 1 # Why is this array the label and not a numeber?\n",
    "\n",
    "            y.append(label)\n",
    "            \n",
    "        print(f\"Loaded {f} class\")\n",
    "\n",
    "X, y = [], []\n",
    "load_data(X, y)\n",
    "\n",
    "# [Q2] Why convert to numpy array?\n",
    "X=np.array(X)\n",
    "y=np.array(y)\n",
    "\n",
    "X = X[:, :,:, 0] # [Q3] Why are we doing this and what does this type of slicing result in?\n",
    "X = X.reshape(X.shape[0], X.shape[1]*X.shape[2]) # [Q4] Why are we reshaping the data?\n",
    "print(\"After reshaping\")\n",
    "print(X.shape, y.shape)\n",
    "print(X[0], y[0])\n",
    "\n",
    "class NN:\n",
    "    def __init__(self, input_neurons, hidden_neurons, output_neurons, learning_rate, epochs):\n",
    "        \"\"\"\n",
    "        Class Definition\n",
    "        \n",
    "        We use a class because it is easy to visualize the process of training a neural network\n",
    "        It's also easier to resuse and repurpose depending on the task at hand\n",
    "\n",
    "        We have a simple neural network, with an input layer, one hidden (middle) layer and an output layer\n",
    "\n",
    "        input_neurons: Number of neurons in the input layer\n",
    "        hidden_neurons: Number of neurons in the hidden layer\n",
    "        output_neurons: Number of neurons in the output layer\n",
    "        learning_rate: The rate at which the weights are updated [Q5] What is the learning rate?\n",
    "        epochs: Number of times the model will train on the entire dataset \n",
    "        \"\"\"\n",
    "\n",
    "        self.input_neurons = input_neurons\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.output_neurons = output_neurons\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        \"\"\"\n",
    "        Weights and Biases\n",
    "\n",
    "        At this point you should know what weights and biases are in a neural network and if not, go check out the 3blue1brown video on Neural Networks\n",
    "        What matters here is however the matrix dimensions of the weights and biases\n",
    "\n",
    "         [Q6] Why are the dimensions of the weights and biases the way they are? \n",
    "        \n",
    "        Try to figure out the dimensions of the weights and biases for the hidden and output layers\n",
    "        Try to see what equations represent the forward pass (basically the prediction)\n",
    "        And then, try to see if the dimensions of the matrix multiplications are correct\n",
    "\n",
    "        Note: The bias dimensions may not match. Look up broadcasting in numpy to understand\n",
    "        [Q7] What is broadcasting and why do we need to broadcast the bias?\n",
    "        \"\"\"\n",
    "\n",
    "        # Ideally any random set of weights and biases can be used to initialize the network\n",
    "        # self.wih = np.random.randn(hidden_neurons, input_neurons)\n",
    "\n",
    "        # [Q8] What is np.random.randn? What's the shape of this matrix?\n",
    "\n",
    "        # Optional: Try to figure out why the weights are initialized this way\n",
    "        # Note: You can just use the commented out line above if you don't want to do this\n",
    "\n",
    "        self.wih = np.random.randn(hidden_neurons, input_neurons) * np.sqrt(2/input_neurons)\n",
    "        self.bih = np.zeros((hidden_neurons, 1))\n",
    "\n",
    "        self.who = np.random.randn(output_neurons, hidden_neurons) * np.sqrt(2/hidden_neurons)\n",
    "        self.bho = np.zeros((output_neurons, 1))\n",
    "\n",
    "    # Activation Functions and their derivatives\n",
    "    # [Q9] What are activation functions and why do we need them?\n",
    "\n",
    "    def relu(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the RELU function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return z * (z > 0)\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the Sigmoid function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def relu_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the RELU derivative function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return 1 * (z > 0)\n",
    "\n",
    "    def sigmoid_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the Sigmoid derivative function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return z * (1 - z)\n",
    "\n",
    "    # [Q10] What is the softmax function and why do we need it? Read up on it\n",
    "    def softmax(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the Softmax function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "\n",
    "    def softmax_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the Softmax derivative function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return z * (1 - z)\n",
    "\n",
    "    # Loss Functions and their derivatives\n",
    "    # [Q11] What are loss functions and why do we need them?\n",
    "\n",
    "    def mean_squared_error(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implementation of the Mean Squared Error function\n",
    "        y: (10, n)\n",
    "        y_hat: (10, n)\n",
    "        returns (1, n)\n",
    "        \"\"\"\n",
    "        return np.mean((y - y_hat) ** 2, axis=0)\n",
    "\n",
    "    def cross_entropy_loss(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implementation of the Cross Entropy Loss function\n",
    "        y: (10, n)\n",
    "        y_hat: (10, n)\n",
    "        returns (1, n)\n",
    "        \"\"\"\n",
    "\n",
    "        # Implement the cross entropy loss function here and return it\n",
    "        # Keep the dimensions of the input in mind when writing the code\n",
    "\n",
    "        # [Code Goes Here]\n",
    "        logits, batch_size = y.shape\n",
    "        loss = []\n",
    "        epsilon = 1e-9  # I got runtime error which I debugged and found out this log(0) problem so had to declare this using help of internet\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            t = 0\n",
    "            for j in range(logits):\n",
    "                t += -y[j, i] * np.log(y_hat[j, i] + epsilon)\n",
    "            loss.append(t)\n",
    "        loss = np.array([loss])\n",
    "        return loss\n",
    "        \n",
    "    def mean_squared_error_derivative(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implementation of the Mean Squared Error derivative function\n",
    "        y: (10, n)\n",
    "        y_hat: (10, n)\n",
    "        returns (10, n)\n",
    "        \"\"\"\n",
    "        return y_hat - y\n",
    "\n",
    "    def cross_entropy_derivative(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implementation of the Cross Entropy Loss derivative function\n",
    "        y: (10, n)\n",
    "        y_hat: (10, n)\n",
    "        returns (10, n)\n",
    "        \"\"\"\n",
    "\n",
    "        # Implement the cross entropy loss derivative function here and return it\n",
    "        # Note: The derivative of the CEL is usually taken with respect to the softmax input not output so keep that in mind while writing\n",
    "\n",
    "        # [Code Goes Here]\n",
    "        cross_entropy_der = y_hat - y\n",
    "\n",
    "        return cross_entropy_der\n",
    "    # Forward propagation\n",
    "    def forward(self, input_list):\n",
    "        \"\"\"\n",
    "        Implementation of the Forward Pass\n",
    "        input_list: (784, n)        - n is the number of images\n",
    "        returns (10, n)              - n is the number of images\n",
    "    \n",
    "        Now we come to the heart of the neural network, the forward pass\n",
    "        This is where the input is passed through the network to get the output\n",
    "\n",
    "        [Q12] What does the output choice we have here mean? It's an array of 10 elements per image, but why?\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = np.array(input_list, ndmin=2).T\n",
    "        inputs = inputs - np.mean(inputs) # [Q13] Why are we subtracting the mean of the inputs?\n",
    "        \n",
    "        # To get to the hidden layer:\n",
    "        # Multiply the input with the weights and adding the bias\n",
    "        # Apply the activation function (relu in this case)\n",
    "\n",
    "        # [Code Goes Here]\n",
    "        z_hidden = np.dot(self.wih, inputs) + self.bih\n",
    "        a_hidden = self.relu(z_hidden)\n",
    "\n",
    "\n",
    "        # To get to the output layer:\n",
    "        # Multiply the hidden layer output with the weights and adding the bias\n",
    "        # Apply the activation function (softmax in this case)\n",
    "        # [Q14] Why are we using the softmax function here?\n",
    "\n",
    "        # [Code Goes Here]\n",
    "        z_output = np.dot(self.who, a_hidden) + self.bho\n",
    "        y_hat = self.softmax(z_output)\n",
    "\n",
    "\n",
    "        # Return it\n",
    "\n",
    "        # [Code Goes Here]\n",
    "        return y_hat\n",
    "        \n",
    "\n",
    "    # Back propagation\n",
    "    def backprop(self, inputs_list, targets_list):\n",
    "        \"\"\"\n",
    "        Implementation of the Backward Pass\n",
    "        inputs_list: (784, n)\n",
    "        targets_list: (10, n)\n",
    "        returns a scalar value (loss)\n",
    "        \n",
    "        This is where the magic happens, the backpropagation algorithm\n",
    "        This is where the weights are updated based on the error in the prediction of the network\n",
    "\n",
    "        Now, the calculus involved is fairly complicated, especially because it's being done in matrix form\n",
    "        However the intuition is simple. \n",
    "\n",
    "        Since this is a recruitment stage, most of the function is written out for you, so follow along with the comments\n",
    "        \"\"\"\n",
    "\n",
    "        # Basic forward pass to get the outputs\n",
    "        # Obviously we need the predictions to know how the model is doing\n",
    "        # [Q15] Why are we doing a forward pass here instead of just using the outputs from the forward function?\n",
    "        # Is there any actual reason, or could we just swap it?\n",
    "\n",
    "        inputs = np.array(inputs_list, ndmin=2).T # (784, n)\n",
    "        inputs = inputs - np.mean(inputs)\n",
    "\n",
    "        tj = np.array(targets_list, ndmin=2).T # (10, n)\n",
    "\n",
    "        hidden_inputs = np.dot(self.wih, inputs) + self.bih\n",
    "        hidden_outputs = self.relu(hidden_inputs)\n",
    "\n",
    "        final_inputs = np.dot(self.who, hidden_outputs) + self.bho\n",
    "        yj = self.softmax(final_inputs)\n",
    "\n",
    "        # Calculating the loss - This is the error in the prediction\n",
    "        # The loss then is the indication of how well the model is doing, its a useful parameter to track to see if the model is improving\n",
    "\n",
    "        loss = self.cross_entropy_loss(tj, yj) # Convert this to cross entropy loss\n",
    "\n",
    "\n",
    "        # Updating the weights using Update Rule\n",
    "        # Now that we have the incorrect predictions, we can update the weights to make the predictions better\n",
    "        # This is done using the gradient of the loss function with respect to the weights\n",
    "        # Basically, we know how much the overall error is caused due to individual weights using the chain rule of calculus\n",
    "        # Since we want to minimise the error, we move in the opposite direction of something like a \"derivative\" of the error with respect to the weights\n",
    "        # Calculus therefore helps us find the direction in which we should move to reduce the error\n",
    "        # A direction means what delta W changes we need to make to make the model better\n",
    "\n",
    "        # Output Layer - We start with the output layer because we are backtracking how the error is caused\n",
    "        # Think of it as using the derivatives of each layer while going back\n",
    "\n",
    "\n",
    "        # For the task, you will be using Cross Entropy Loss\n",
    "\n",
    "        # Change this to cross entropy loss\n",
    "        dE_dzo = self.mean_squared_error_derivative(tj, yj) * self.softmax_derivative(yj) # (10,n)\n",
    "        # Note: the derivative of the CEL is usually taken with respect to the softmax input not output so keep that in mind while writing\n",
    "\n",
    "\n",
    "        dE_dwho = np.dot(dE_dzo, hidden_outputs.T) / hidden_outputs.shape[1] # dot((10,n) (n,128) = (10,128)\n",
    "        dE_dbho = np.mean(dE_dzo, axis=1, keepdims=True) # sum((10,n), axis=1) = (10,1)\n",
    "        \n",
    "        self.who -= self.lr * dE_dwho\n",
    "        self.bho -= self.lr * dE_dbho\n",
    "\n",
    "        # Hidden Layer\n",
    "        dE_dah = np.dot(self.who.T, dE_dzo) # dot((128,10), (10,n)) = (128,n)\n",
    "        dE_dzh = dE_dah * self.relu_derivative(hidden_inputs)\n",
    "        dE_dwih = np.dot(dE_dzh, inputs.T) / inputs.shape[1]\n",
    "        dE_dbih = np.mean(dE_dzh, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "        self.wih -= self.lr * dE_dwih\n",
    "        self.bih -= self.lr * dE_dbih\n",
    "\n",
    "        return np.mean(loss)\n",
    "\n",
    "    def fit(self, inputs_list, targets_list,validation_data, validation_labels):\n",
    "        \"\"\"\n",
    "        Implementation of the training loop\n",
    "        inputs_list: (784, n)\n",
    "        targets_list: (10, n)\n",
    "        validation_data: (784, n)\n",
    "        validation_labels: (10, n)\n",
    "        returns train_loss, val_loss\n",
    "\n",
    "        This is where the training loop is implemented\n",
    "        We loop over the entire dataset for a certain number of epochs\n",
    "        We also track the validation loss to see how well the model is generalizing\n",
    "        [Q16] What is the validation dataset and what do we mean by generalization?\n",
    "\n",
    "        We also return the training and validation loss to see how the model is improving\n",
    "        It's a good idea to plot these to see how the model is doing\n",
    "        \"\"\"\n",
    "\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "        for epoch in range(self.epochs):\n",
    "            loss = self.backprop(inputs_list, targets_list)\n",
    "            train_loss.append(loss)\n",
    "            vloss = self.mean_squared_error(validation_labels.T, self.forward(validation_data))\n",
    "            val_loss.append(np.mean(vloss)) \n",
    "            print(f\"Epoch: {epoch}, Loss: {loss}, Val Loss: {val_loss[-1]}\")\n",
    "\n",
    "        return train_loss[1:], val_loss[:-1] \n",
    "\n",
    "    def predict(self, X):\n",
    "        outputs = self.forward(X).T\n",
    "        return outputs\n",
    "\n",
    "# This is where the class is used to train the model\n",
    "\n",
    "# The parameters in the model are (input_neurons, hidden_neurons, output_neurons, learning_rate, epochs)\n",
    "# These parameters aren't the right parameters, so tweak them to get the best results\n",
    "# Around 70% accuracy is a good end goal (75% is great) but for the recruitment task, 60% is good enough\n",
    "\n",
    "# [Q17] What are the parameters in the model and what do they mean?\n",
    "\n",
    "fashion_mnist = NN(784, 160, 10, 0.03, 130)\n",
    "p = np.random.permutation(len(X))\n",
    "X, y = X[p], y[p]\n",
    "\n",
    "# Splitting the data into training, validation and testing in the ratio 70:20:10\n",
    "X_train, y_train = X[:int(0.7*len(X))], y[:int(0.7*len(X))]\n",
    "X_val, y_val = X[int(0.7*len(X)):int(0.9*len(X))], y[int(0.7*len(X)):int(0.9*len(X))]\n",
    "X_test, y_test = X[int(0.9*len(X)):], y[int(0.9*len(X)):]\n",
    "\n",
    "# Training the model\n",
    "train_loss,val_loss = fashion_mnist.fit(X_train, y_train,X_val,y_val)\n",
    "\n",
    "\n",
    "# Plotting the loss\n",
    "plt.plot(train_loss,label='train')\n",
    "plt.plot(val_loss,label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred = fashion_mnist.predict(X_test)\n",
    "\n",
    "# [Q18] Why are we using argmax here? Why is this output different from the output of the model?\n",
    "y_pred = np.argmax(y_pred, axis=1)  \n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "print(f\"Accuracy: {np.mean(y_pred == y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8317357,
     "sourceId": 13129288,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 451.10115,
   "end_time": "2025-09-21T19:49:16.531044",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-21T19:41:45.429894",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
